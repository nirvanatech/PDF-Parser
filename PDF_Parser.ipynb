{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67f46632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pypdf extraction and OpenAI classification script\n",
    "# import pandas as pd\n",
    "# import pypdf\n",
    "# import os\n",
    "# import json\n",
    "# import time\n",
    "# import logging\n",
    "# from openai import OpenAI # Use the updated OpenAI library import\n",
    "# from typing import Dict, Any\n",
    "\n",
    "# # === LOGGING SETUP ===\n",
    "# # Sets up basic configuration for logging messages.\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # === CONFIGURATION ===\n",
    "# # It's better to manage paths and model names in a central place.\n",
    "# # The API key is read from a local file for better security.\n",
    "# API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\" # <--- ADJUST IF NEEDED\n",
    "# MODEL = \"gpt-4o\" # Using the latest model as specified\n",
    "\n",
    "# def get_openai_client(api_key_path: str) -> OpenAI:\n",
    "#     \"\"\"Reads the OpenAI API key from a file and returns an OpenAI client.\"\"\"\n",
    "#     try:\n",
    "#         with open(api_key_path, 'r') as f:\n",
    "#             api_key = f.read().strip()\n",
    "#         if not api_key:\n",
    "#             raise ValueError(\"API key file is empty.\")\n",
    "#         logger.info(\"Successfully loaded OpenAI API key.\")\n",
    "#         return OpenAI(api_key=api_key)\n",
    "#     except FileNotFoundError:\n",
    "#         logger.error(f\"API key file not found at: {api_key_path}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while reading the API key: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Extracts all text from a given PDF file, page by page.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(pdf_path):\n",
    "#         logger.error(f\"Error: File not found at {pdf_path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     logger.info(f\"Starting text extraction from '{os.path.basename(pdf_path)}'...\")\n",
    "#     all_pages_data = []\n",
    "\n",
    "#     try:\n",
    "#         with open(pdf_path, 'rb') as file:\n",
    "#             reader = pypdf.PdfReader(file)\n",
    "#             num_pages = len(reader.pages)\n",
    "#             logger.info(f\"Found {num_pages} pages in the document.\")\n",
    "\n",
    "#             for i, page in enumerate(reader.pages):\n",
    "#                 page_number = i + 1\n",
    "#                 text = page.extract_text() or \"\" # Ensure text is a string\n",
    "                \n",
    "#                 all_pages_data.append({\n",
    "#                     'page_number': page_number,\n",
    "#                     'text': text.strip()\n",
    "#                 })\n",
    "#                 if not text.strip():\n",
    "#                     logger.info(f\"  - No text found on page {page_number}.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while processing the PDF: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     if all_pages_data:\n",
    "#         df = pd.DataFrame(all_pages_data)\n",
    "#         logger.info(\"Text extraction complete.\")\n",
    "#         return df\n",
    "#     else:\n",
    "#         logger.warning(\"Warning: No text was extracted from the document.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# def classify_page_text(client: OpenAI, page_number: int, page_text: str) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Classifies the text of a single page using the OpenAI API.\n",
    "#     \"\"\"\n",
    "#     # The new system prompt with updated instructions and JSON format.\n",
    "#     # Note: The model is only asked for classification details.\n",
    "#     # Page number and text are added back into the final dictionary later.\n",
    "#     sys_prompt = \"\"\"\n",
    "# You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "\n",
    "# Your job is to classify each page into a single best-fitting category (\"bucket\"). The following buckets are examples of likely categories, but you are allowed to invent and assign a new, appropriate bucket name, if the existing examples do not fit.\n",
    "\n",
    "# BUCKET EXAMPLES (use or invent as needed):\n",
    "\n",
    "# - intro information: Cover letters, summaries, company info, administrative headers.\n",
    "# - table of contents: Tables/indexes listing sections, rules, forms.\n",
    "# - correspondence: Letters, memos, formal or informal communication (including with a regulator).\n",
    "# - rule: Detailed rating rules, eligibility, underwriting guidelines, standard operating instructions.\n",
    "# - factor table: Tabular lists of rating factors—e.g. for zones, territories, drivers, vehicles.\n",
    "# - actuarial support: Mathematical or statistical justification, trend documentation, loss ratios, exhibits.\n",
    "# - form: Complete forms, endorsements, schedules, specimen policy wordings.\n",
    "# - rating example: A worked example showing how premium/rate is calculated.\n",
    "# - exhibit: Graphs, charts, additional annotated attachments or appendices.\n",
    "# - crossed_out: (binary) Use ONLY if the entire page is covered with a line, annotated \"withdrawn,\" or has visible strikethrough/crossout. Otherwise, do not use.\n",
    "# - other: Use only if the page fits none of the above and you cannot reasonably propose a more accurate new bucket name.\n",
    "\n",
    "# BUCKET FLEXIBILITY:\n",
    "# - If none of the above buckets are a good fit, make up an appropriate, concise, descriptive bucket name and use it as the \"bucket\". Do NOT use \"llm_new_category\" as a category name—use your proposed name directly (e.g. \"signature page\", \"state certification\", etc).\n",
    "\n",
    "# CATEGORIZATION INSTRUCTIONS:\n",
    "# - Assign exactly one bucket per page.\n",
    "# - Always provide a 10 word \"explanation\" of why you selected—or if new, created—this bucket.\n",
    "# - If \"crossed_out\" is chosen, no substantive explanation is needed—just state \"Entire page was striked out or withdrawn.\"\n",
    "# - Otherwise, explain the dominant content and your reasoning for the bucket chosen in precisely 10 words.\n",
    "\n",
    "# OUTPUT FORMAT (respond with a single valid JSON object only):\n",
    "\n",
    "# {\n",
    "#   \"bucket\": \"<bucket_name>\",\n",
    "#   \"confidence\": <probability 0-1>,\n",
    "#   \"explanation\": \"<10 word explanation of categorization>\"\n",
    "# }\n",
    "\n",
    "# If uncertain, favor \"other\", but prefer to create (with reasoned explanation) a new appropriate bucket when justified.\n",
    "# \"\"\"\n",
    "#     # Create the full result dictionary here, starting with known values.\n",
    "#     # This ensures page_number and text are always present, even on error.\n",
    "#     result_payload = {\n",
    "#         \"page_number\": page_number,\n",
    "#         \"bucket\": \"processing_error\",\n",
    "#         \"confidence\": 0.0,\n",
    "#         \"explanation\": \"An error occurred before the API call.\",\n",
    "#         \"text\": page_text\n",
    "#     }\n",
    "\n",
    "#     if not page_text:\n",
    "#         result_payload.update({\n",
    "#             \"bucket\": \"other\",\n",
    "#             \"confidence\": 1.0,\n",
    "#             \"explanation\": \"Page is blank or contains no extractable text.\"\n",
    "#         })\n",
    "#         return result_payload\n",
    "\n",
    "#     for attempt in range(3): # Retry logic for transient API errors\n",
    "#         try:\n",
    "#             chat_completion = client.chat.completions.create(\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": sys_prompt},\n",
    "#                     {\"role\": \"user\", \"content\": page_text[:16000]}, # Increased token limit for gpt-4o\n",
    "#                 ],\n",
    "#                 model=MODEL,\n",
    "#                 response_format={\"type\": \"json_object\"},\n",
    "#                 temperature=0.0,\n",
    "#             )\n",
    "#             response_content = chat_completion.choices[0].message.content\n",
    "#             # Parse the JSON from the model\n",
    "#             api_result = json.loads(response_content)\n",
    "#             # Update the payload with the model's response\n",
    "#             result_payload.update(api_result)\n",
    "#             return result_payload\n",
    "#         except Exception as e:\n",
    "#             logger.warning(f\"API call failed on attempt {attempt + 1} for page {page_number}: {e}. Retrying in {2 ** attempt}s...\")\n",
    "#             time.sleep(2 ** attempt)\n",
    "\n",
    "#     logger.error(f\"API call failed after multiple retries for page {page_number}.\")\n",
    "#     result_payload.update({\n",
    "#         \"bucket\": \"api_error\",\n",
    "#         \"explanation\": \"API call failed after multiple retries.\"\n",
    "#     })\n",
    "#     return result_payload\n",
    "\n",
    "# # --- Main Script Execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # --- USER INPUT ---\n",
    "#     input_pdf_path = \"./Inputs/PGR_Ohio_BNIC-134120828_trimmed.pdf\"\n",
    "#     output_csv_path = \"./Output/classified_pdf_text_aiv2.csv\"\n",
    "\n",
    "#     # --- SCRIPT LOGIC ---\n",
    "#     # 1. Initialize OpenAI Client\n",
    "#     openai_client = get_openai_client(API_KEY_PATH)\n",
    "    \n",
    "#     if openai_client:\n",
    "#         # 2. Extract text from PDF\n",
    "#         pdf_dataframe = extract_text_from_pdf(input_pdf_path)\n",
    "\n",
    "#         if not pdf_dataframe.empty:\n",
    "#             # 3. Classify each page\n",
    "#             logger.info(\"Starting page classification process...\")\n",
    "            \n",
    "#             results = []\n",
    "#             total_pages = len(pdf_dataframe)\n",
    "#             for index, row in pdf_dataframe.iterrows():\n",
    "#                 logger.info(f\"Classifying page {row['page_number']}/{total_pages}...\")\n",
    "#                 # Pass page number and text to the classification function\n",
    "#                 result = classify_page_text(\n",
    "#                     client=openai_client, \n",
    "#                     page_number=row['page_number'], \n",
    "#                     page_text=row['text']\n",
    "#                 )\n",
    "#                 results.append(result)\n",
    "\n",
    "#             # 4. Create the final DataFrame from the list of result dictionaries\n",
    "#             final_df = pd.DataFrame(results)\n",
    "\n",
    "#             # 5. Save the final results to CSV\n",
    "#             try:\n",
    "#                 # Reorder columns for clarity in the output CSV\n",
    "#                 column_order = ['page_number', 'bucket', 'confidence', 'explanation', 'text']\n",
    "#                 final_df = final_df[column_order]\n",
    "                \n",
    "#                 final_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "#                 logger.info(f\"\\nSuccessfully saved classified text to '{output_csv_path}'\")\n",
    "                \n",
    "#                 logger.info(\"\\n--- Sample of Final Data (text column omitted for brevity) ---\")\n",
    "#                 print(final_df.drop(columns=['text']).head().to_string())\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"\\nAn error occurred while saving the CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b3a3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pypdf extraction and OpenAI classification script\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from openai import OpenAI # Use the updated OpenAI library import\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "# Sets up basic configuration for logging messages.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# It's better to manage paths and model names in a central place.\n",
    "# The API key is read from a local file for better security.\n",
    "API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\" # <--- ADJUST IF NEEDED\n",
    "MODEL = \"gpt-4o\" # Using the latest model as specified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_openai_client(api_key_path: str) -> OpenAI:\n",
    "    \"\"\"Reads the OpenAI API key from a file and returns an OpenAI client.\"\"\"\n",
    "    try:\n",
    "        with open(api_key_path, 'r') as f:\n",
    "            api_key = f.read().strip()\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key file is empty.\")\n",
    "        logger.info(\"Successfully loaded OpenAI API key.\")\n",
    "        return OpenAI(api_key=api_key)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"API key file not found at: {api_key_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while reading the API key: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts all text from a given PDF file, page by page.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        logger.error(f\"Error: File not found at {pdf_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Starting text extraction from '{os.path.basename(pdf_path)}'...\")\n",
    "    all_pages_data = []\n",
    "\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = pypdf.PdfReader(file)\n",
    "            num_pages = len(reader.pages)\n",
    "            logger.info(f\"Found {num_pages} pages in the document.\")\n",
    "\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                page_number = i + 1\n",
    "                text = page.extract_text() or \"\" # Ensure text is a string\n",
    "                \n",
    "                all_pages_data.append({\n",
    "                    'page_number': page_number,\n",
    "                    'text': text.strip()\n",
    "                })\n",
    "                if not text.strip():\n",
    "                    logger.info(f\"  - No text found on page {page_number}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while processing the PDF: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if all_pages_data:\n",
    "        df = pd.DataFrame(all_pages_data)\n",
    "        logger.info(\"Text extraction complete.\")\n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(\"Warning: No text was extracted from the document.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_company_name(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the company name from the first few pages of the document.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return \"Unknown_Company\"\n",
    "    \n",
    "    # Check the first 3 pages for the company name for robustness\n",
    "    for i in range(min(3, len(df))):\n",
    "        page_text = df.iloc[i]['text']\n",
    "        # Use regex to find \"Filing Company:\" and capture the text after it\n",
    "        match = re.search(r\"Filing Company:\\s*(.*)\", page_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            company_name = match.group(1).strip()\n",
    "            # A second regex to clean up any trailing text like \"Project Name/Number\"\n",
    "            company_name = re.split(r'\\s{2,}|Project Name', company_name)[0].strip()\n",
    "            if company_name:\n",
    "                return company_name\n",
    "    \n",
    "    logger.warning(\"Could not find company name. Defaulting to 'Unknown_Company'.\")\n",
    "    return \"Unknown_Company\"\n",
    "\n",
    "\n",
    "def classify_page_text(client: OpenAI, page_number: int, page_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classifies the text of a single page using rules and the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Create the base payload. This ensures all keys are present, even on error.\n",
    "    result_payload = {\n",
    "        \"page_number\": page_number,\n",
    "        \"bucket\": \"processing_error\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"explanation\": \"An error occurred during processing.\",\n",
    "        \"text\": page_text\n",
    "    }\n",
    "\n",
    "    # RULE #1: Handle blank pages first.\n",
    "    if not page_text:\n",
    "        result_payload.update({\n",
    "            \"bucket\": \"other\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"explanation\": \"Page is blank or contains no extractable text.\"\n",
    "        })\n",
    "        return result_payload\n",
    "\n",
    "    # RULE #2: Handle \"redline\" pages before calling the API.\n",
    "    if 'redline' in page_text.lower():\n",
    "        result_payload.update({\n",
    "            \"bucket\": \"redline\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"explanation\": \"Page contains 'redline' text, indicating document revisions.\"\n",
    "        })\n",
    "        return result_payload\n",
    "\n",
    "    # If no rules match, proceed with API call.\n",
    "    sys_prompt = \"\"\"\n",
    "You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "\n",
    "Your job is to classify each page into a single best-fitting category (\"bucket\"). The following buckets are examples of likely categories, but you are allowed to invent and assign a new, appropriate bucket name, if the existing examples do not fit.\n",
    "\n",
    "BUCKET EXAMPLES (use or invent as needed):\n",
    "\n",
    "- intro information: Cover letters, summaries, company info, administrative headers.\n",
    "- table of contents: Tables/indexes listing sections, rules, forms.\n",
    "- correspondence: Letters, memos, formal or informal communication (including with a regulator).\n",
    "- rule: Detailed rating rules, eligibility, underwriting guidelines, standard operating instructions.\n",
    "- factor table: Tabular lists of rating factors—e.g. for zones, territories, drivers, vehicles.\n",
    "- actuarial support: Mathematical or statistical justification, trend documentation, loss ratios, exhibits.\n",
    "- form: Complete forms, endorsements, schedules, specimen policy wordings.\n",
    "- rating example: A worked example showing how premium/rate is calculated.\n",
    "- exhibit: Graphs, charts, additional annotated attachments or appendices.\n",
    "- crossed_out: (binary) Use ONLY if the entire page is covered with a line, annotated \"withdrawn,\" or has visible strikrough/crossout. Otherwise, do not use.\n",
    "- other: Use only if the page fits none of the above and you cannot reasonably propose a more accurate new bucket name.\n",
    "\n",
    "BUCKET FLEXIBILITY:\n",
    "- If none of the above buckets are a good fit, make up an appropriate, concise, descriptive bucket name and use it as the \"bucket\". Do NOT use \"llm_new_category\" as a category name—use your proposed name directly (e.g. \"signature page\", \"state certification\", etc).\n",
    "\n",
    "CATEGORIZATION INSTRUCTIONS:\n",
    "- Assign exactly one bucket per page.\n",
    "- Always provide a 10 word \"explanation\" of why you selected—or if new, created—this bucket.\n",
    "- If \"crossed_out\" is chosen, no substantive explanation is needed—just state \"Entire page was striked out or withdrawn.\"\n",
    "- Otherwise, explain the dominant content and your reasoning for the bucket chosen in precisely 10 words.\n",
    "\n",
    "OUTPUT FORMAT (respond with a single valid JSON object only):\n",
    "\n",
    "{\n",
    "  \"bucket\": \"<bucket_name>\",\n",
    "  \"confidence\": <probability 0-1>,\n",
    "  \"explanation\": \"<10 word explanation of categorization>\"\n",
    "}\n",
    "\n",
    "If uncertain, favor \"other\", but prefer to create (with reasoned explanation) a new appropriate bucket when justified.\n",
    "\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": page_text[:16000]},\n",
    "                ],\n",
    "                model=MODEL,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            response_content = chat_completion.choices[0].message.content\n",
    "            api_result = json.loads(response_content)\n",
    "            result_payload.update(api_result)\n",
    "            return result_payload\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"API call failed on attempt {attempt + 1} for page {page_number}: {e}. Retrying in {2 ** attempt}s...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "    logger.error(f\"API call failed after multiple retries for page {page_number}.\")\n",
    "    result_payload.update({\n",
    "        \"bucket\": \"api_error\",\n",
    "        \"explanation\": \"API call failed after multiple retries.\"\n",
    "    })\n",
    "    return result_payload\n",
    "\n",
    "def parse_single_page_tables(client: OpenAI, page_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses the AI to extract all tables from a SINGLE page's text.\n",
    "    \"\"\"\n",
    "    sys_prompt = \"\"\"\n",
    "You are an expert data extraction assistant. Your task is to analyze the provided text from a SINGLE document page and extract ALL tables present.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1.  **Identify ALL Distinct Tables**: Scrutinize the text to identify every individual table. A new table is often indicated by a new title or header row (e.g., \"Table 2-1...\").\n",
    "2.  **Handle Continuations**: The text might contain the start of a new table, the end of a table from a previous page, or both. Parse what is present. If a block of text looks like table rows but has no header, treat it as a table and extract the rows as-is.\n",
    "3.  **Extract Titles**: For each distinct table you identify, find its title. If a table has no clear title, create a concise, descriptive one. If it's a continuation, the title might be absent; in this case, use a placeholder like \"Continuation Table\".\n",
    "4.  **Recreate Each Table**: For each table, parse the text to reconstruct its data, including headers (if present) and all data rows.\n",
    "5.  **Return a List of JSON Objects**: Your output must be a single, valid JSON object containing a list of all tables found on the page.\n",
    "\n",
    "EXAMPLE OUTPUT for a page with two tables:\n",
    "{\n",
    "  \"tables\": [\n",
    "    { \"table_title\": \"Table 1-2. Average Driver Experience Score\", \"table_data\": [[\"Greater than\", \"Factor\"], [\"0\", \"0.989\"]] },\n",
    "    { \"table_title\": \"Table 2-1. Number of Violations\", \"table_data\": [[\"Violations\", \"Factor\"], [\"0\", \"1.000\"]] }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": page_text},\n",
    "            ],\n",
    "            model=MODEL,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "        return json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during single-page table parsing: {e}\")\n",
    "        return {\"tables\": []}\n",
    "\n",
    "def is_header_row(row: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristically determines if a list of strings is a table header.\n",
    "    Returns False if it's likely a data row.\n",
    "    \"\"\"\n",
    "    # A simple heuristic: if a row contains mostly numbers, it's likely data.\n",
    "    # This can be made more sophisticated if needed.\n",
    "    numeric_count = 0\n",
    "    for item in row:\n",
    "        try:\n",
    "            float(str(item).replace(\",\", \"\").replace(\"$\", \"\"))\n",
    "            numeric_count += 1\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    # If more than half the cells are numeric, it's probably not a header.\n",
    "    return numeric_count <= len(row) / 2\n",
    "\n",
    "def combine_table_fragments(parsed_tables: List[Tuple[str, pd.DataFrame]]) -> List[Tuple[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Combines a list of parsed table DataFrames into consolidated tables.\n",
    "    \"\"\"\n",
    "    if not parsed_tables:\n",
    "        return []\n",
    "\n",
    "    logger.info(\"Combining table fragments...\")\n",
    "    combined_tables = []\n",
    "    \n",
    "    # Start with the first table as the base\n",
    "    current_title, current_df = parsed_tables[0]\n",
    "    \n",
    "    for i in range(1, len(parsed_tables)):\n",
    "        next_title, next_df = parsed_tables[i]\n",
    "        \n",
    "        # If the next DataFrame is empty or has no rows, skip it\n",
    "        if next_df.empty or len(next_df.columns) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if the first row of the next table looks like a header\n",
    "        first_row_is_header = is_header_row(list(next_df.columns))\n",
    "\n",
    "        # If the next table has a new header, it's a new table.\n",
    "        # Finalize the current table and start a new one.\n",
    "        if first_row_is_header:\n",
    "            combined_tables.append((current_title, current_df))\n",
    "            current_title, current_df = next_title, next_df\n",
    "        # If it looks like a continuation (no header), append it.\n",
    "        else:\n",
    "            # Rename columns of the fragment to match the base table\n",
    "            if len(current_df.columns) == len(next_df.columns):\n",
    "                next_df.columns = current_df.columns\n",
    "                current_df = pd.concat([current_df, next_df], ignore_index=True)\n",
    "            else:\n",
    "                logger.warning(f\"Column mismatch for table '{current_title}'. Cannot combine fragment. Starting new table.\")\n",
    "                combined_tables.append((current_title, current_df))\n",
    "                current_title, current_df = next_title, next_df\n",
    "\n",
    "    # Add the last processed table\n",
    "    combined_tables.append((current_title, current_df))\n",
    "    \n",
    "    logger.info(f\"Consolidated into {len(combined_tables)} final tables.\")\n",
    "    return combined_tables\n",
    "\n",
    "# === STEP 1: PDF Parsing and Page Classification ===\n",
    "def run_classification_step(input_pdf_path: str, output_csv_path: str) -> (pd.DataFrame, str):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, classifies each page, and saves the results to a CSV.\n",
    "    Returns the classification DataFrame and the extracted company name.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Step 1: PDF Parsing and Page Classification ---\")\n",
    "    openai_client = get_openai_client(API_KEY_PATH)\n",
    "    if not openai_client:\n",
    "        return pd.DataFrame(), \"Unknown_Company\"\n",
    "\n",
    "    pdf_dataframe = extract_text_from_pdf(input_pdf_path)\n",
    "    if pdf_dataframe.empty:\n",
    "        return pd.DataFrame(), \"Unknown_Company\"\n",
    "\n",
    "    company_name = extract_company_name(pdf_dataframe)\n",
    "    logger.info(f\"Extracted Company Name: {company_name}\")\n",
    "\n",
    "    results = []\n",
    "    total_pages = len(pdf_dataframe)\n",
    "    for index, row in pdf_dataframe.iterrows():\n",
    "        logger.info(f\"Classifying page {row['page_number']}/{total_pages}...\")\n",
    "        result = classify_page_text(\n",
    "            client=openai_client, \n",
    "            page_number=row['page_number'], \n",
    "            page_text=row['text']\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    final_df = pd.DataFrame(results)\n",
    "    \n",
    "    try:\n",
    "        column_order = ['page_number', 'bucket', 'confidence', 'explanation', 'text']\n",
    "        final_df_ordered = final_df.reindex(columns=column_order)\n",
    "        final_df_ordered.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"\\nSuccessfully saved classification results to '{output_csv_path}'\")\n",
    "        logger.info(\"\\n--- Sample of Classification Data (text column omitted for brevity) ---\")\n",
    "        print(final_df_ordered.drop(columns=['text']).head().to_string())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\nAn error occurred while saving the CSV file: {e}\")\n",
    "\n",
    "    return final_df, company_name\n",
    "\n",
    "# === STEP 2: Table Extraction and Structuring ===\n",
    "def run_table_extraction_step(classified_df: pd.DataFrame, company_name: str, output_excel_path: str):\n",
    "    \"\"\"\n",
    "    Processes the classified data to find and extract factor tables into an Excel file.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Starting Step 2: Factor Table Extraction ---\")\n",
    "    if classified_df.empty:\n",
    "        logger.warning(\"Classification DataFrame is empty. Skipping table extraction.\")\n",
    "        return\n",
    "        \n",
    "    openai_client = get_openai_client(API_KEY_PATH)\n",
    "    if not openai_client:\n",
    "        return\n",
    "        \n",
    "    factor_table_pages = classified_df[\n",
    "        classified_df['bucket'].str.lower().isin(['factor_table', 'factor table'])\n",
    "    ].copy()\n",
    "    \n",
    "    if factor_table_pages.empty:\n",
    "        logger.info(\"No pages were classified as 'factor_table'. Skipping table extraction.\")\n",
    "        return\n",
    "        \n",
    "    # --- New Step 2a: Parse each factor table page individually ---\n",
    "    all_parsed_tables = []\n",
    "    for index, row in factor_table_pages.iterrows():\n",
    "        page_num = row['page_number']\n",
    "        logger.info(f\"Parsing tables on page: {page_num}\")\n",
    "        parsed_data = parse_single_page_tables(openai_client, row['text'])\n",
    "        for table in parsed_data.get(\"tables\", []):\n",
    "            table_title = table.get(\"table_title\", f\"Table_on_Page_{page_num}\")\n",
    "            table_data = table.get(\"table_data\", [])\n",
    "            if table_data and len(table_data) > 0: # Check if there is at least a header\n",
    "                try:\n",
    "                    header = table_data[0]\n",
    "                    data = table_data[1:]\n",
    "                    \n",
    "                    # FIX: Sanitize the data to ensure all rows have the same number of columns as the header\n",
    "                    num_columns = len(header)\n",
    "                    sanitized_data = []\n",
    "                    for data_row in data:\n",
    "                        # Pad rows that are too short\n",
    "                        while len(data_row) < num_columns:\n",
    "                            data_row.append('')\n",
    "                        # Truncate rows that are too long\n",
    "                        sanitized_data.append(data_row[:num_columns])\n",
    "\n",
    "                    table_df = pd.DataFrame(sanitized_data, columns=header)\n",
    "                    all_parsed_tables.append((table_title, table_df))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Could not process table '{table_title}' on page {page_num} due to data structure issue: {e}\")\n",
    "\n",
    "\n",
    "    # --- New Step 2b: Combine the parsed fragments ---\n",
    "    final_tables = combine_table_fragments(all_parsed_tables)\n",
    "\n",
    "    # --- Step 2c: Write the final, combined tables to Excel ---\n",
    "    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "        sheet_name_counts = {}\n",
    "        for title, df in final_tables:\n",
    "            full_title = f\"{company_name} - {title}\"\n",
    "            safe_sheet_name = \"\".join(c for c in full_title if c.isalnum() or c in (' ', '_')).rstrip()[:25]\n",
    "            \n",
    "            if safe_sheet_name in sheet_name_counts:\n",
    "                sheet_name_counts[safe_sheet_name] += 1\n",
    "                final_sheet_name = f\"{safe_sheet_name}_{sheet_name_counts[safe_sheet_name]}\"\n",
    "            else:\n",
    "                sheet_name_counts[safe_sheet_name] = 0\n",
    "                final_sheet_name = safe_sheet_name\n",
    "            \n",
    "            final_sheet_name = final_sheet_name[:31]\n",
    "\n",
    "            df.to_excel(writer, sheet_name=final_sheet_name, index=False)\n",
    "            logger.info(f\"  - Saved table '{full_title}' to sheet '{final_sheet_name}'\")\n",
    "\n",
    "    logger.info(f\"Successfully saved extracted tables to '{output_excel_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aeaf17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 12:16:11,482 | INFO | Input PDF: ./202505 - TX - PRGS-134279210 _ trimmed.pdf\n",
      "2025-08-27 12:16:11,483 | INFO | Output directory: ./Output/202505 - TX - PRGS-134279210 _ trimmed\n",
      "2025-08-27 12:16:11,483 | INFO | --- Skipping Step 1, Loading pre-classified data from: /Users/jake/Documents/Python/PDF Parser/Output/202505 - TX - PRGS-134279210 _ trimmed/classified_pages_trimmed.csv ---\n",
      "2025-08-27 12:16:11,491 | WARNING | Could not find company name. Defaulting to 'Unknown_Company'.\n",
      "2025-08-27 12:16:11,492 | INFO | Extracted Company Name from CSV: Unknown_Company\n",
      "2025-08-27 12:16:11,493 | INFO | \n",
      "--- Starting Step 2: Factor Table Extraction ---\n",
      "2025-08-27 12:16:11,494 | INFO | Successfully loaded OpenAI API key.\n",
      "2025-08-27 12:16:11,527 | INFO | Parsing tables on page: 21\n",
      "2025-08-27 12:16:20,479 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:16:20,485 | INFO | Parsing tables on page: 22\n",
      "2025-08-27 12:16:49,815 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:16:49,820 | INFO | Parsing tables on page: 23\n",
      "2025-08-27 12:17:23,485 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:17:23,489 | INFO | Parsing tables on page: 24\n",
      "2025-08-27 12:17:52,269 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:17:52,274 | INFO | Parsing tables on page: 25\n",
      "2025-08-27 12:18:19,744 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:18:19,748 | INFO | Parsing tables on page: 26\n",
      "2025-08-27 12:18:47,796 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:18:47,803 | INFO | Parsing tables on page: 27\n",
      "2025-08-27 12:19:12,754 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:19:12,759 | INFO | Parsing tables on page: 28\n",
      "2025-08-27 12:19:40,099 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:19:40,104 | INFO | Parsing tables on page: 29\n",
      "2025-08-27 12:20:10,914 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:20:10,918 | INFO | Parsing tables on page: 30\n",
      "2025-08-27 12:20:36,463 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:20:36,467 | INFO | Parsing tables on page: 31\n",
      "2025-08-27 12:21:03,650 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:21:03,654 | INFO | Parsing tables on page: 32\n",
      "2025-08-27 12:21:30,259 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:21:30,263 | INFO | Parsing tables on page: 33\n",
      "2025-08-27 12:21:58,487 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:21:58,490 | INFO | Parsing tables on page: 34\n",
      "2025-08-27 12:22:31,061 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:22:31,065 | INFO | Parsing tables on page: 35\n",
      "2025-08-27 12:23:30,432 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:23:30,437 | INFO | Parsing tables on page: 36\n",
      "2025-08-27 12:24:00,777 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:24:00,847 | INFO | Parsing tables on page: 284\n",
      "2025-08-27 12:24:19,567 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:24:19,571 | INFO | Parsing tables on page: 285\n",
      "2025-08-27 12:24:34,804 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:24:34,809 | INFO | Parsing tables on page: 286\n",
      "2025-08-27 12:24:52,251 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:24:52,258 | INFO | Parsing tables on page: 287\n",
      "2025-08-27 12:25:07,108 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:25:07,112 | INFO | Parsing tables on page: 288\n",
      "2025-08-27 12:25:25,952 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:25:25,955 | INFO | Parsing tables on page: 289\n",
      "2025-08-27 12:25:42,733 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:25:42,737 | INFO | Parsing tables on page: 290\n",
      "2025-08-27 12:26:00,171 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:26:00,176 | INFO | Parsing tables on page: 291\n",
      "2025-08-27 12:26:15,592 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:26:15,634 | INFO | Parsing tables on page: 292\n",
      "2025-08-27 12:26:31,635 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:26:31,638 | INFO | Parsing tables on page: 293\n",
      "2025-08-27 12:26:47,604 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:26:47,609 | INFO | Parsing tables on page: 294\n",
      "2025-08-27 12:27:04,941 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:27:04,947 | INFO | Parsing tables on page: 295\n",
      "2025-08-27 12:27:20,885 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:27:20,890 | INFO | Parsing tables on page: 296\n",
      "2025-08-27 12:27:36,894 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:27:36,899 | INFO | Parsing tables on page: 297\n",
      "2025-08-27 12:27:56,424 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:27:56,430 | INFO | Parsing tables on page: 298\n",
      "2025-08-27 12:28:12,042 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:28:12,045 | INFO | Parsing tables on page: 299\n",
      "2025-08-27 12:28:27,459 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:28:27,468 | INFO | Parsing tables on page: 300\n",
      "2025-08-27 12:28:41,996 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:28:42,000 | INFO | Parsing tables on page: 301\n",
      "2025-08-27 12:28:57,896 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:28:57,901 | INFO | Parsing tables on page: 302\n",
      "2025-08-27 12:29:13,994 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:29:14,001 | INFO | Parsing tables on page: 303\n",
      "2025-08-27 12:29:29,849 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:29:29,853 | INFO | Parsing tables on page: 304\n",
      "2025-08-27 12:29:44,665 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 12:29:44,673 | INFO | Parsing tables on page: 305\n",
      "2025-08-27 13:10:27,981 | INFO | Retrying request to /chat/completions in 0.398788 seconds\n",
      "2025-08-27 13:10:43,873 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:10:43,882 | INFO | Parsing tables on page: 306\n",
      "2025-08-27 13:11:00,017 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:11:00,021 | INFO | Parsing tables on page: 307\n",
      "2025-08-27 13:11:15,324 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:11:15,329 | INFO | Parsing tables on page: 308\n",
      "2025-08-27 13:11:28,653 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:11:28,656 | INFO | Parsing tables on page: 309\n",
      "2025-08-27 13:11:43,475 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:11:43,485 | INFO | Parsing tables on page: 310\n",
      "2025-08-27 13:11:59,272 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:11:59,277 | INFO | Parsing tables on page: 311\n",
      "2025-08-27 13:12:15,097 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:12:15,100 | INFO | Parsing tables on page: 312\n",
      "2025-08-27 13:12:32,569 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:12:32,574 | INFO | Parsing tables on page: 313\n",
      "2025-08-27 13:12:47,320 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:12:47,327 | INFO | Parsing tables on page: 314\n",
      "2025-08-27 13:13:03,245 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:13:03,252 | INFO | Parsing tables on page: 315\n",
      "2025-08-27 13:13:18,322 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:13:18,326 | INFO | Parsing tables on page: 316\n",
      "2025-08-27 13:13:34,154 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:13:34,158 | INFO | Parsing tables on page: 317\n",
      "2025-08-27 13:13:49,369 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:13:49,373 | INFO | Parsing tables on page: 318\n",
      "2025-08-27 13:14:05,731 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-27 13:14:05,739 | INFO | Combining table fragments...\n",
      "2025-08-27 13:14:05,741 | INFO | Consolidated into 51 final tables.\n",
      "2025-08-27 13:14:05,758 | INFO |   - Saved table 'Unknown_Company - Body Type Group Assignment Table' to sheet 'Unknown_Company  Body Typ'\n",
      "2025-08-27 13:14:05,765 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business'\n",
      "2025-08-27 13:14:05,771 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_1'\n",
      "2025-08-27 13:14:05,775 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_2'\n",
      "2025-08-27 13:14:05,780 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_3'\n",
      "2025-08-27 13:14:05,784 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_4'\n",
      "2025-08-27 13:14:05,788 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_5'\n",
      "2025-08-27 13:14:05,793 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_6'\n",
      "2025-08-27 13:14:05,796 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_7'\n",
      "2025-08-27 13:14:05,800 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_8'\n",
      "2025-08-27 13:14:05,804 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_9'\n",
      "2025-08-27 13:14:05,808 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_10'\n",
      "2025-08-27 13:14:05,811 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_11'\n",
      "2025-08-27 13:14:05,816 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_12'\n",
      "2025-08-27 13:14:05,819 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_13'\n",
      "2025-08-27 13:14:05,822 | INFO |   - Saved table 'Unknown_Company - Business Class Body Type Use Factors' to sheet 'Unknown_Company  Business_14'\n",
      "2025-08-27 13:14:05,829 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye'\n",
      "2025-08-27 13:14:05,833 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_1'\n",
      "2025-08-27 13:14:05,836 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_2'\n",
      "2025-08-27 13:14:05,839 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_3'\n",
      "2025-08-27 13:14:05,842 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_4'\n",
      "2025-08-27 13:14:05,848 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_5'\n",
      "2025-08-27 13:14:05,851 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_6'\n",
      "2025-08-27 13:14:05,853 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_7'\n",
      "2025-08-27 13:14:05,856 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_8'\n",
      "2025-08-27 13:14:05,859 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_9'\n",
      "2025-08-27 13:14:05,862 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_10'\n",
      "2025-08-27 13:14:05,864 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_11'\n",
      "2025-08-27 13:14:05,867 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_12'\n",
      "2025-08-27 13:14:05,869 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_13'\n",
      "2025-08-27 13:14:05,873 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_14'\n",
      "2025-08-27 13:14:05,875 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_15'\n",
      "2025-08-27 13:14:05,877 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_16'\n",
      "2025-08-27 13:14:05,880 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_17'\n",
      "2025-08-27 13:14:05,882 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_18'\n",
      "2025-08-27 13:14:05,884 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_19'\n",
      "2025-08-27 13:14:05,885 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_20'\n",
      "2025-08-27 13:14:05,887 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_21'\n",
      "2025-08-27 13:14:05,890 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_22'\n",
      "2025-08-27 13:14:05,892 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_23'\n",
      "2025-08-27 13:14:05,893 | INFO |   - Saved table 'Unknown_Company - Model Year Factor Table' to sheet 'Unknown_Company  Model Ye_24'\n",
      "2025-08-27 13:14:05,896 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage '\n",
      "2025-08-27 13:14:05,898 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _1'\n",
      "2025-08-27 13:14:05,900 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _2'\n",
      "2025-08-27 13:14:05,902 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _3'\n",
      "2025-08-27 13:14:05,904 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _4'\n",
      "2025-08-27 13:14:05,906 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _5'\n",
      "2025-08-27 13:14:05,909 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _6'\n",
      "2025-08-27 13:14:05,911 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _7'\n",
      "2025-08-27 13:14:05,913 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _8'\n",
      "2025-08-27 13:14:05,915 | INFO |   - Saved table 'Unknown_Company - Mileage Factor Table' to sheet 'Unknown_Company  Mileage _9'\n",
      "2025-08-27 13:14:06,012 | INFO | Successfully saved extracted tables to './Output/202505 - TX - PRGS-134279210 _ trimmed/extracted_factor_tables_pgr_trimmed.xlsx'\n",
      "2025-08-27 13:14:06,013 | INFO | \n",
      "--- Script finished ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- USER INPUT ---\n",
    "    input_pdf_path = \"./202505 - TX - PRGS-134279210 _ trimmed.pdf\"\n",
    "    # To skip the classification step and use an existing CSV, provide the path here.\n",
    "    # Otherwise, leave it as None.\n",
    "    # Example: pre_classified_csv_path = \"./Output/PGR_Ohio_BNIC-134120828/classified_pages.csv\"\n",
    "    pre_classified_csv_path = \"/Users/jake/Documents/Python/PDF Parser/Output/202505 - TX - PRGS-134279210 _ trimmed/classified_pages_trimmed.csv\" # <--- SET THIS TO A CSV PATH TO SKIP STEP 1\n",
    "\n",
    "    # --- DYNAMIC OUTPUT PATHS ---\n",
    "    base_filename = os.path.basename(input_pdf_path)\n",
    "    file_name_without_ext = os.path.splitext(base_filename)[0]\n",
    "    \n",
    "    # 1. Create a dedicated output subfolder for the file\n",
    "    output_dir = os.path.join(\"./Output\", file_name_without_ext)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define output paths inside the new subfolder\n",
    "    output_csv_path = os.path.join(output_dir, f\"classified_pages.csv\")\n",
    "    output_excel_path = os.path.join(output_dir, f\"extracted_factor_tables_pgr_trimmed.xlsx\")\n",
    "    \n",
    "    logger.info(f\"Input PDF: {input_pdf_path}\")\n",
    "    logger.info(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    # --- SCRIPT LOGIC ---\n",
    "    # 2. Run the two main steps sequentially, with an option to skip Step 1\n",
    "    \n",
    "    # Check if a pre-classified CSV should be used\n",
    "    if pre_classified_csv_path and os.path.exists(pre_classified_csv_path):\n",
    "        logger.info(f\"--- Skipping Step 1, Loading pre-classified data from: {pre_classified_csv_path} ---\")\n",
    "        classified_data = pd.read_csv(pre_classified_csv_path)\n",
    "        # Handle potential NaN values in the 'text' column when loading from CSV\n",
    "        classified_data['text'] = classified_data['text'].fillna('')\n",
    "        company = extract_company_name(classified_data)\n",
    "        logger.info(f\"Extracted Company Name from CSV: {company}\")\n",
    "    else:\n",
    "        if pre_classified_csv_path:\n",
    "            logger.warning(f\"Pre-classified file not found at '{pre_classified_csv_path}'. Running full classification process.\")\n",
    "        # Run the full classification step if no pre-classified file is provided\n",
    "        classified_data, company = run_classification_step(input_pdf_path, output_csv_path)\n",
    "\n",
    "    # Run the table extraction step using the (either newly created or loaded) classified data\n",
    "    if not classified_data.empty:\n",
    "        run_table_extraction_step(classified_data, company, output_excel_path)\n",
    "    else:\n",
    "        logger.error(\"Classification data is empty. Cannot proceed to table extraction.\")\n",
    "\n",
    "    logger.info(\"\\n--- Script finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3eb49",
   "metadata": {},
   "source": [
    "    sys_prompt = \"\"\"\n",
    "You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "\n",
    "Your job is to classify each page into a single best-fitting category (\"bucket\"). The following buckets are examples of likely categories, but you are allowed to invent and assign a new, appropriate bucket name, if the existing examples do not fit.\n",
    "\n",
    "BUCKET EXAMPLES (use or invent as needed):\n",
    "\n",
    "- intro information: Cover letters, summaries, company info, administrative headers.\n",
    "- table of contents: Tables/indexes listing sections, rules, forms.\n",
    "- correspondence: Letters, memos, formal or informal communication (including with a regulator).\n",
    "- rule: Detailed rating rules, eligibility, underwriting guidelines, standard operating instructions.\n",
    "- factor table: Tabular lists of rating factors—e.g. for zones, territories, drivers, vehicles.\n",
    "- actuarial support: Mathematical or statistical justification, trend documentation, loss ratios, exhibits.\n",
    "- form: Complete forms, endorsements, schedules, specimen policy wordings.\n",
    "- rating example: A worked example showing how premium/rate is calculated.\n",
    "- exhibit: Graphs, charts, additional annotated attachments or appendices.\n",
    "- crossed_out: (binary) Use ONLY if the entire page is covered with a line, annotated \"withdrawn,\" or has visible strikrough/crossout. Otherwise, do not use.\n",
    "- other: Use only if the page fits none of the above and you cannot reasonably propose a more accurate new bucket name.\n",
    "\n",
    "BUCKET FLEXIBILITY:\n",
    "- If none of the above buckets are a good fit, make up an appropriate, concise, descriptive bucket name and use it as the \"bucket\". Do NOT use \"llm_new_category\" as a category name—use your proposed name directly (e.g. \"signature page\", \"state certification\", etc).\n",
    "\n",
    "CATEGORIZATION INSTRUCTIONS:\n",
    "- Assign exactly one bucket per page.\n",
    "- Always provide a 10 word \"explanation\" of why you selected—or if new, created—this bucket.\n",
    "- If \"crossed_out\" is chosen, no substantive explanation is needed—just state \"Entire page was striked out or withdrawn.\"\n",
    "- Otherwise, explain the dominant content and your reasoning for the bucket chosen in precisely 10 words.\n",
    "\n",
    "OUTPUT FORMAT (respond with a single valid JSON object only):\n",
    "\n",
    "{\n",
    "  \"bucket\": \"<bucket_name>\",\n",
    "  \"confidence\": <probability 0-1>,\n",
    "  \"explanation\": \"<10 word explanation of categorization>\"\n",
    "}\n",
    "\n",
    "If uncertain, favor \"other\", but prefer to create (with reasoned explanation) a new appropriate bucket when justified.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
