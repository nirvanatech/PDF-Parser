{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f46632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pypdf extraction and OpenAI classification script\n",
    "# import pandas as pd\n",
    "# import pypdf\n",
    "# import os\n",
    "# import json\n",
    "# import time\n",
    "# import logging\n",
    "# from openai import OpenAI # Use the updated OpenAI library import\n",
    "# from typing import Dict, Any\n",
    "\n",
    "# # === LOGGING SETUP ===\n",
    "# # Sets up basic configuration for logging messages.\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # === CONFIGURATION ===\n",
    "# # It's better to manage paths and model names in a central place.\n",
    "# # The API key is read from a local file for better security.\n",
    "# API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\" # <--- ADJUST IF NEEDED\n",
    "# MODEL = \"gpt-4o\" # Using the latest model as specified\n",
    "\n",
    "# def get_openai_client(api_key_path: str) -> OpenAI:\n",
    "#     \"\"\"Reads the OpenAI API key from a file and returns an OpenAI client.\"\"\"\n",
    "#     try:\n",
    "#         with open(api_key_path, 'r') as f:\n",
    "#             api_key = f.read().strip()\n",
    "#         if not api_key:\n",
    "#             raise ValueError(\"API key file is empty.\")\n",
    "#         logger.info(\"Successfully loaded OpenAI API key.\")\n",
    "#         return OpenAI(api_key=api_key)\n",
    "#     except FileNotFoundError:\n",
    "#         logger.error(f\"API key file not found at: {api_key_path}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while reading the API key: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Extracts all text from a given PDF file, page by page.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(pdf_path):\n",
    "#         logger.error(f\"Error: File not found at {pdf_path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     logger.info(f\"Starting text extraction from '{os.path.basename(pdf_path)}'...\")\n",
    "#     all_pages_data = []\n",
    "\n",
    "#     try:\n",
    "#         with open(pdf_path, 'rb') as file:\n",
    "#             reader = pypdf.PdfReader(file)\n",
    "#             num_pages = len(reader.pages)\n",
    "#             logger.info(f\"Found {num_pages} pages in the document.\")\n",
    "\n",
    "#             for i, page in enumerate(reader.pages):\n",
    "#                 page_number = i + 1\n",
    "#                 text = page.extract_text() or \"\" # Ensure text is a string\n",
    "                \n",
    "#                 all_pages_data.append({\n",
    "#                     'page_number': page_number,\n",
    "#                     'text': text.strip()\n",
    "#                 })\n",
    "#                 if not text.strip():\n",
    "#                     logger.info(f\"  - No text found on page {page_number}.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred while processing the PDF: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     if all_pages_data:\n",
    "#         df = pd.DataFrame(all_pages_data)\n",
    "#         logger.info(\"Text extraction complete.\")\n",
    "#         return df\n",
    "#     else:\n",
    "#         logger.warning(\"Warning: No text was extracted from the document.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# def classify_page_text(client: OpenAI, page_number: int, page_text: str) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Classifies the text of a single page using the OpenAI API.\n",
    "#     \"\"\"\n",
    "#     # The new system prompt with updated instructions and JSON format.\n",
    "#     # Note: The model is only asked for classification details.\n",
    "#     # Page number and text are added back into the final dictionary later.\n",
    "#     sys_prompt = \"\"\"\n",
    "# You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "\n",
    "# Your job is to classify each page into a single best-fitting category (\"bucket\"). The following buckets are examples of likely categories, but you are allowed to invent and assign a new, appropriate bucket name, if the existing examples do not fit.\n",
    "\n",
    "# BUCKET EXAMPLES (use or invent as needed):\n",
    "\n",
    "# - intro information: Cover letters, summaries, company info, administrative headers.\n",
    "# - table of contents: Tables/indexes listing sections, rules, forms.\n",
    "# - correspondence: Letters, memos, formal or informal communication (including with a regulator).\n",
    "# - rule: Detailed rating rules, eligibility, underwriting guidelines, standard operating instructions.\n",
    "# - factor table: Tabular lists of rating factors—e.g. for zones, territories, drivers, vehicles.\n",
    "# - actuarial support: Mathematical or statistical justification, trend documentation, loss ratios, exhibits.\n",
    "# - form: Complete forms, endorsements, schedules, specimen policy wordings.\n",
    "# - rating example: A worked example showing how premium/rate is calculated.\n",
    "# - exhibit: Graphs, charts, additional annotated attachments or appendices.\n",
    "# - crossed_out: (binary) Use ONLY if the entire page is covered with a line, annotated \"withdrawn,\" or has visible strikethrough/crossout. Otherwise, do not use.\n",
    "# - other: Use only if the page fits none of the above and you cannot reasonably propose a more accurate new bucket name.\n",
    "\n",
    "# BUCKET FLEXIBILITY:\n",
    "# - If none of the above buckets are a good fit, make up an appropriate, concise, descriptive bucket name and use it as the \"bucket\". Do NOT use \"llm_new_category\" as a category name—use your proposed name directly (e.g. \"signature page\", \"state certification\", etc).\n",
    "\n",
    "# CATEGORIZATION INSTRUCTIONS:\n",
    "# - Assign exactly one bucket per page.\n",
    "# - Always provide a 10 word \"explanation\" of why you selected—or if new, created—this bucket.\n",
    "# - If \"crossed_out\" is chosen, no substantive explanation is needed—just state \"Entire page was striked out or withdrawn.\"\n",
    "# - Otherwise, explain the dominant content and your reasoning for the bucket chosen in precisely 10 words.\n",
    "\n",
    "# OUTPUT FORMAT (respond with a single valid JSON object only):\n",
    "\n",
    "# {\n",
    "#   \"bucket\": \"<bucket_name>\",\n",
    "#   \"confidence\": <probability 0-1>,\n",
    "#   \"explanation\": \"<10 word explanation of categorization>\"\n",
    "# }\n",
    "\n",
    "# If uncertain, favor \"other\", but prefer to create (with reasoned explanation) a new appropriate bucket when justified.\n",
    "# \"\"\"\n",
    "#     # Create the full result dictionary here, starting with known values.\n",
    "#     # This ensures page_number and text are always present, even on error.\n",
    "#     result_payload = {\n",
    "#         \"page_number\": page_number,\n",
    "#         \"bucket\": \"processing_error\",\n",
    "#         \"confidence\": 0.0,\n",
    "#         \"explanation\": \"An error occurred before the API call.\",\n",
    "#         \"text\": page_text\n",
    "#     }\n",
    "\n",
    "#     if not page_text:\n",
    "#         result_payload.update({\n",
    "#             \"bucket\": \"other\",\n",
    "#             \"confidence\": 1.0,\n",
    "#             \"explanation\": \"Page is blank or contains no extractable text.\"\n",
    "#         })\n",
    "#         return result_payload\n",
    "\n",
    "#     for attempt in range(3): # Retry logic for transient API errors\n",
    "#         try:\n",
    "#             chat_completion = client.chat.completions.create(\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": sys_prompt},\n",
    "#                     {\"role\": \"user\", \"content\": page_text[:16000]}, # Increased token limit for gpt-4o\n",
    "#                 ],\n",
    "#                 model=MODEL,\n",
    "#                 response_format={\"type\": \"json_object\"},\n",
    "#                 temperature=0.0,\n",
    "#             )\n",
    "#             response_content = chat_completion.choices[0].message.content\n",
    "#             # Parse the JSON from the model\n",
    "#             api_result = json.loads(response_content)\n",
    "#             # Update the payload with the model's response\n",
    "#             result_payload.update(api_result)\n",
    "#             return result_payload\n",
    "#         except Exception as e:\n",
    "#             logger.warning(f\"API call failed on attempt {attempt + 1} for page {page_number}: {e}. Retrying in {2 ** attempt}s...\")\n",
    "#             time.sleep(2 ** attempt)\n",
    "\n",
    "#     logger.error(f\"API call failed after multiple retries for page {page_number}.\")\n",
    "#     result_payload.update({\n",
    "#         \"bucket\": \"api_error\",\n",
    "#         \"explanation\": \"API call failed after multiple retries.\"\n",
    "#     })\n",
    "#     return result_payload\n",
    "\n",
    "# # --- Main Script Execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # --- USER INPUT ---\n",
    "#     input_pdf_path = \"./Inputs/PGR_Ohio_BNIC-134120828_trimmed.pdf\"\n",
    "#     output_csv_path = \"./Output/classified_pdf_text_aiv2.csv\"\n",
    "\n",
    "#     # --- SCRIPT LOGIC ---\n",
    "#     # 1. Initialize OpenAI Client\n",
    "#     openai_client = get_openai_client(API_KEY_PATH)\n",
    "    \n",
    "#     if openai_client:\n",
    "#         # 2. Extract text from PDF\n",
    "#         pdf_dataframe = extract_text_from_pdf(input_pdf_path)\n",
    "\n",
    "#         if not pdf_dataframe.empty:\n",
    "#             # 3. Classify each page\n",
    "#             logger.info(\"Starting page classification process...\")\n",
    "            \n",
    "#             results = []\n",
    "#             total_pages = len(pdf_dataframe)\n",
    "#             for index, row in pdf_dataframe.iterrows():\n",
    "#                 logger.info(f\"Classifying page {row['page_number']}/{total_pages}...\")\n",
    "#                 # Pass page number and text to the classification function\n",
    "#                 result = classify_page_text(\n",
    "#                     client=openai_client, \n",
    "#                     page_number=row['page_number'], \n",
    "#                     page_text=row['text']\n",
    "#                 )\n",
    "#                 results.append(result)\n",
    "\n",
    "#             # 4. Create the final DataFrame from the list of result dictionaries\n",
    "#             final_df = pd.DataFrame(results)\n",
    "\n",
    "#             # 5. Save the final results to CSV\n",
    "#             try:\n",
    "#                 # Reorder columns for clarity in the output CSV\n",
    "#                 column_order = ['page_number', 'bucket', 'confidence', 'explanation', 'text']\n",
    "#                 final_df = final_df[column_order]\n",
    "                \n",
    "#                 final_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "#                 logger.info(f\"\\nSuccessfully saved classified text to '{output_csv_path}'\")\n",
    "                \n",
    "#                 logger.info(\"\\n--- Sample of Final Data (text column omitted for brevity) ---\")\n",
    "#                 print(final_df.drop(columns=['text']).head().to_string())\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"\\nAn error occurred while saving the CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09e1411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pypdf extraction and OpenAI classification script\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from openai import OpenAI # Use the updated OpenAI library import\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "# Sets up basic configuration for logging messages.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# It's better to manage paths and model names in a central place.\n",
    "# The API key is read from a local file for better security.\n",
    "API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\" # <--- ADJUST IF NEEDED\n",
    "MODEL = \"gpt-4o\" # Using the latest model as specified\n",
    "# Maximum characters to send in a single table extraction request to avoid token limits.\n",
    "# gpt-4o's context window is large, but prompts/responses also consume tokens. \n",
    "# 100,000 chars is a safe starting point (approx. 25k tokens).\n",
    "MAX_CHARS_PER_GROUP = 100000 \n",
    "\n",
    "\n",
    "def get_openai_client(api_key_path: str) -> OpenAI:\n",
    "    \"\"\"Reads the OpenAI API key from a file and returns an OpenAI client.\"\"\"\n",
    "    try:\n",
    "        with open(api_key_path, 'r') as f:\n",
    "            api_key = f.read().strip()\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key file is empty.\")\n",
    "        logger.info(\"Successfully loaded OpenAI API key.\")\n",
    "        return OpenAI(api_key=api_key)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"API key file not found at: {api_key_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while reading the API key: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts all text from a given PDF file, page by page.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        logger.error(f\"Error: File not found at {pdf_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Starting text extraction from '{os.path.basename(pdf_path)}'...\")\n",
    "    all_pages_data = []\n",
    "\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = pypdf.PdfReader(file)\n",
    "            num_pages = len(reader.pages)\n",
    "            logger.info(f\"Found {num_pages} pages in the document.\")\n",
    "\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                page_number = i + 1\n",
    "                text = page.extract_text() or \"\" # Ensure text is a string\n",
    "                \n",
    "                all_pages_data.append({\n",
    "                    'page_number': page_number,\n",
    "                    'text': text.strip()\n",
    "                })\n",
    "                if not text.strip():\n",
    "                    logger.info(f\"  - No text found on page {page_number}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while processing the PDF: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if all_pages_data:\n",
    "        df = pd.DataFrame(all_pages_data)\n",
    "        logger.info(\"Text extraction complete.\")\n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(\"Warning: No text was extracted from the document.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_company_name(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the company name from the first few pages of the document.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return \"Unknown_Company\"\n",
    "    \n",
    "    # Check the first 3 pages for the company name for robustness\n",
    "    for i in range(min(3, len(df))):\n",
    "        page_text = df.iloc[i]['text']\n",
    "        # Use regex to find \"Filing Company:\" and capture the text after it\n",
    "        match = re.search(r\"Filing Company:\\s*(.*)\", page_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            company_name = match.group(1).strip()\n",
    "            # A second regex to clean up any trailing text like \"Project Name/Number\"\n",
    "            company_name = re.split(r'\\s{2,}|Project Name', company_name)[0].strip()\n",
    "            if company_name:\n",
    "                return company_name\n",
    "    \n",
    "    logger.warning(\"Could not find company name. Defaulting to 'Unknown_Company'.\")\n",
    "    return \"Unknown_Company\"\n",
    "\n",
    "\n",
    "def classify_page_text(client: OpenAI, page_number: int, page_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classifies the text of a single page using rules and the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Create the base payload. This ensures all keys are present, even on error.\n",
    "    result_payload = {\n",
    "        \"page_number\": page_number,\n",
    "        \"bucket\": \"processing_error\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"explanation\": \"An error occurred during processing.\",\n",
    "        \"text\": page_text\n",
    "    }\n",
    "\n",
    "    # RULE #1: Handle blank pages first.\n",
    "    if not page_text:\n",
    "        result_payload.update({\n",
    "            \"bucket\": \"other\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"explanation\": \"Page is blank or contains no extractable text.\"\n",
    "        })\n",
    "        return result_payload\n",
    "\n",
    "    # RULE #2: Handle \"redline\" pages before calling the API.\n",
    "    if 'redline' in page_text.lower():\n",
    "        result_payload.update({\n",
    "            \"bucket\": \"redline\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"explanation\": \"Page contains 'redline' text, indicating document revisions.\"\n",
    "        })\n",
    "        return result_payload\n",
    "\n",
    "    # If no rules match, proceed with API call.\n",
    "    # **FIX**: Restored the full system prompt to include the word \"JSON\", which is required\n",
    "    # by the API when using response_format=\"json_object\".\n",
    "    sys_prompt = \"\"\"\n",
    "You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "\n",
    "Your job is to classify each page into a single best-fitting category (\"bucket\"). The following buckets are examples of likely categories, but you are allowed to invent and assign a new, appropriate bucket name, if the existing examples do not fit.\n",
    "\n",
    "BUCKET EXAMPLES (use or invent as needed):\n",
    "\n",
    "- intro information: Cover letters, summaries, company info, administrative headers.\n",
    "- table of contents: Tables/indexes listing sections, rules, forms.\n",
    "- correspondence: Letters, memos, formal or informal communication (including with a regulator).\n",
    "- rule: Detailed rating rules, eligibility, underwriting guidelines, standard operating instructions.\n",
    "- factor table: Tabular lists of rating factors—e.g. for zones, territories, drivers, vehicles.\n",
    "- actuarial support: Mathematical or statistical justification, trend documentation, loss ratios, exhibits.\n",
    "- form: Complete forms, endorsements, schedules, specimen policy wordings.\n",
    "- rating example: A worked example showing how premium/rate is calculated.\n",
    "- exhibit: Graphs, charts, additional annotated attachments or appendices.\n",
    "- crossed_out: (binary) Use ONLY if the entire page is covered with a line, annotated \"withdrawn,\" or has visible strikrough/crossout. Otherwise, do not use.\n",
    "- other: Use only if the page fits none of the above and you cannot reasonably propose a more accurate new bucket name.\n",
    "\n",
    "BUCKET FLEXIBILITY:\n",
    "- If none of the above buckets are a good fit, make up an appropriate, concise, descriptive bucket name and use it as the \"bucket\". Do NOT use \"llm_new_category\" as a category name—use your proposed name directly (e.g. \"signature page\", \"state certification\", etc).\n",
    "\n",
    "CATEGORIZATION INSTRUCTIONS:\n",
    "- Assign exactly one bucket per page.\n",
    "- Always provide a 10 word \"explanation\" of why you selected—or if new, created—this bucket.\n",
    "- If \"crossed_out\" is chosen, no substantive explanation is needed—just state \"Entire page was striked out or withdrawn.\"\n",
    "- Otherwise, explain the dominant content and your reasoning for the bucket chosen in precisely 10 words.\n",
    "\n",
    "OUTPUT FORMAT (respond with a single valid JSON object only):\n",
    "\n",
    "{\n",
    "  \"bucket\": \"<bucket_name>\",\n",
    "  \"confidence\": <probability 0-1>,\n",
    "  \"explanation\": \"<10 word explanation of categorization>\"\n",
    "}\n",
    "\n",
    "If uncertain, favor \"other\", but prefer to create (with reasoned explanation) a new appropriate bucket when justified.\n",
    "\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": page_text[:16000]},\n",
    "                ],\n",
    "                model=MODEL,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            response_content = chat_completion.choices[0].message.content\n",
    "            api_result = json.loads(response_content)\n",
    "            result_payload.update(api_result)\n",
    "            return result_payload\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"API call failed on attempt {attempt + 1} for page {page_number}: {e}. Retrying in {2 ** attempt}s...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "    logger.error(f\"API call failed after multiple retries for page {page_number}.\")\n",
    "    result_payload.update({\n",
    "        \"bucket\": \"api_error\",\n",
    "        \"explanation\": \"API call failed after multiple retries.\"\n",
    "    })\n",
    "    return result_payload\n",
    "\n",
    "def extract_and_structure_table(client: OpenAI, table_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses the AI to extract titles and structured data from text containing one or more tables.\n",
    "    \"\"\"\n",
    "    sys_prompt = \"\"\"\n",
    "You are an expert data extraction assistant. Your task is to analyze the provided text, which may contain multiple, separate factor tables from an insurance filing, potentially spanning page breaks.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1.  **Identify ALL Distinct Tables**: Scrutinize the entire text to identify every individual table. A new table is often indicated by a new title or header row (e.g., \"Table 2-1...\").\n",
    "2.  **Handle Multi-Page Tables**: A table might start on one page and continue on the next. A page break is indicated by \"--- NEW PAGE BREAK ---\". If a page starts with data rows and no new header, it is a continuation of the previous table. You must intelligently stitch these parts together into a single table.\n",
    "3.  **Extract Titles**: For each distinct table you identify, find its title. If a table has no clear title, create a concise, descriptive one based on its content.\n",
    "4.  **Recreate Each Table**: For each distinct table, parse the text to reconstruct its data, including its specific headers and all corresponding data rows.\n",
    "5.  **Return a List of JSON Objects**: Your output must be a single, valid JSON object that contains a list of all the tables you found. Each table in the list should have the following structure:\n",
    "    {\n",
    "      \"table_title\": \"<The title you identified or created for this specific table>\",\n",
    "      \"table_data\": [\n",
    "        [\"Header 1\", \"Header 2\", \"Header 3\"],\n",
    "        [\"Row 1 Col 1\", \"Row 1 Col 2\", \"Row 1 Col 3\"],\n",
    "        [\"Row 2 Col 1\", \"Row 2 Col 2\", \"Row 2 Col 3\"]\n",
    "      ]\n",
    "    }\n",
    "\n",
    "EXAMPLE OUTPUT for text containing two separate tables:\n",
    "{\n",
    "  \"tables\": [\n",
    "    {\n",
    "      \"table_title\": \"Table 1-2. Average Driver Experience Score\",\n",
    "      \"table_data\": [\n",
    "        [\"Greater than\", \"And less than or equal to\", \"Factor\"],\n",
    "        [\"0\", \"0.986\", \"0.989\"],\n",
    "        [\"0.986\", \"0.998\", \"1.000\"]\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"table_title\": \"Table 2-1. Number of Super & Major Violations\",\n",
    "      \"table_data\": [\n",
    "        [\"Number of Super & Major Violations\", \"Factor\"],\n",
    "        [\"0\", \"1.000\"],\n",
    "        [\"1\", \"1.354\"]\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": table_text},\n",
    "            ],\n",
    "            model=MODEL,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        response_content = chat_completion.choices[0].message.content\n",
    "        return json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during table extraction: {e}\")\n",
    "        return {\n",
    "            \"tables\": [{\n",
    "                \"table_title\": \"Extraction Error\",\n",
    "                \"table_data\": [[f\"An error occurred: {e}\"]]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "def process_table_group(client: OpenAI, df: pd.DataFrame, group_indices: List[int], writer: pd.ExcelWriter, company_name: str, sheet_name_counts: dict):\n",
    "    \"\"\"\n",
    "    Processes a single group of consecutive factor table pages, expecting multiple tables.\n",
    "    \"\"\"\n",
    "    page_numbers = df.loc[group_indices, 'page_number'].tolist()\n",
    "    logger.info(f\"Processing a potential table group spanning pages: {page_numbers}\")\n",
    "    \n",
    "    combined_text = \"\\n--- NEW PAGE BREAK ---\\n\".join(df.loc[group_indices, 'text'])\n",
    "    \n",
    "    structured_data = extract_and_structure_table(client, combined_text)\n",
    "    \n",
    "    tables = structured_data.get(\"tables\", [])\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        table_title = table.get(\"table_title\", f\"Untitled_Table_{i+1}\")\n",
    "        table_data = table.get(\"table_data\", [])\n",
    "        \n",
    "        if table_data and len(table_data) > 1:\n",
    "            full_title = f\"{company_name} - {table_title}\"\n",
    "            safe_sheet_name = \"\".join(c for c in full_title if c.isalnum() or c in (' ', '_')).rstrip()[:25]\n",
    "            \n",
    "            if safe_sheet_name in sheet_name_counts:\n",
    "                sheet_name_counts[safe_sheet_name] += 1\n",
    "                final_sheet_name = f\"{safe_sheet_name}_{sheet_name_counts[safe_sheet_name]}\"\n",
    "            else:\n",
    "                sheet_name_counts[safe_sheet_name] = 0\n",
    "                final_sheet_name = safe_sheet_name\n",
    "            \n",
    "            final_sheet_name = final_sheet_name[:31]\n",
    "\n",
    "            table_df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "            table_df.to_excel(writer, sheet_name=final_sheet_name, index=False)\n",
    "            logger.info(f\"  - Saved table '{full_title}' to sheet '{final_sheet_name}'\")\n",
    "        else:\n",
    "            logger.warning(f\"  - Skipping empty or invalid table data for title: '{table_title}'\")\n",
    "\n",
    "# === STEP 1: PDF Parsing and Page Classification ===\n",
    "def run_classification_step(input_pdf_path: str, output_csv_path: str) -> (pd.DataFrame, str):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, classifies each page, and saves the results to a CSV.\n",
    "    Returns the classification DataFrame and the extracted company name.\n",
    "    \"\"\"\n",
    "    logger.info(\"--- Starting Step 1: PDF Parsing and Page Classification ---\")\n",
    "    openai_client = get_openai_client(API_KEY_PATH)\n",
    "    if not openai_client:\n",
    "        return pd.DataFrame(), \"Unknown_Company\"\n",
    "\n",
    "    pdf_dataframe = extract_text_from_pdf(input_pdf_path)\n",
    "    if pdf_dataframe.empty:\n",
    "        return pd.DataFrame(), \"Unknown_Company\"\n",
    "\n",
    "    company_name = extract_company_name(pdf_dataframe)\n",
    "    logger.info(f\"Extracted Company Name: {company_name}\")\n",
    "\n",
    "    results = []\n",
    "    total_pages = len(pdf_dataframe)\n",
    "    for index, row in pdf_dataframe.iterrows():\n",
    "        logger.info(f\"Classifying page {row['page_number']}/{total_pages}...\")\n",
    "        result = classify_page_text(\n",
    "            client=openai_client, \n",
    "            page_number=row['page_number'], \n",
    "            page_text=row['text']\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    final_df = pd.DataFrame(results)\n",
    "    \n",
    "    try:\n",
    "        column_order = ['page_number', 'bucket', 'confidence', 'explanation', 'text']\n",
    "        final_df_ordered = final_df.reindex(columns=column_order)\n",
    "        final_df_ordered.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"\\nSuccessfully saved classification results to '{output_csv_path}'\")\n",
    "        logger.info(\"\\n--- Sample of Classification Data (text column omitted for brevity) ---\")\n",
    "        print(final_df_ordered.drop(columns=['text']).head().to_string())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\nAn error occurred while saving the CSV file: {e}\")\n",
    "\n",
    "    return final_df, company_name\n",
    "\n",
    "# === STEP 2: Table Extraction and Structuring ===\n",
    "def run_table_extraction_step(classified_df: pd.DataFrame, company_name: str, output_excel_path: str):\n",
    "    \"\"\"\n",
    "    Processes the classified data to find and extract factor tables into an Excel file.\n",
    "    \"\"\"\n",
    "    logger.info(\"\\n--- Starting Step 2: Factor Table Extraction ---\")\n",
    "    if classified_df.empty:\n",
    "        logger.warning(\"Classification DataFrame is empty. Skipping table extraction.\")\n",
    "        return\n",
    "        \n",
    "    openai_client = get_openai_client(API_KEY_PATH)\n",
    "    if not openai_client:\n",
    "        return\n",
    "        \n",
    "    factor_table_pages = classified_df[\n",
    "        classified_df['bucket'].str.lower().isin(['factor_table', 'factor table'])\n",
    "    ]\n",
    "    \n",
    "    if factor_table_pages.empty:\n",
    "        logger.info(\"No pages were classified as 'factor_table'. Skipping table extraction.\")\n",
    "        return\n",
    "\n",
    "    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "        page_indices = factor_table_pages.index.tolist()\n",
    "        if not page_indices:\n",
    "            return\n",
    "            \n",
    "        sheet_name_counts = {}\n",
    "        current_group = []\n",
    "        current_chars = 0\n",
    "        \n",
    "        for index in page_indices:\n",
    "            # Start a new group if this page isn't consecutive with the last one\n",
    "            if current_group and index != current_group[-1] + 1:\n",
    "                process_table_group(openai_client, classified_df, current_group, writer, company_name, sheet_name_counts)\n",
    "                current_group = []\n",
    "                current_chars = 0\n",
    "            \n",
    "            # Add the page to the current group\n",
    "            page_text_len = len(classified_df.loc[index, 'text'])\n",
    "            \n",
    "            # If adding this page exceeds the char limit, process the current group first\n",
    "            if current_group and (current_chars + page_text_len) > MAX_CHARS_PER_GROUP:\n",
    "                process_table_group(openai_client, classified_df, current_group, writer, company_name, sheet_name_counts)\n",
    "                current_group = []\n",
    "                current_chars = 0\n",
    "\n",
    "            current_group.append(index)\n",
    "            current_chars += page_text_len\n",
    "        \n",
    "        # Process the final remaining group\n",
    "        if current_group:\n",
    "            process_table_group(openai_client, classified_df, current_group, writer, company_name, sheet_name_counts)\n",
    "\n",
    "    logger.info(f\"Successfully saved extracted tables to '{output_excel_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaf17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 11:00:53,175 | INFO | Input PDF: ./202505 - TX - PRGS-134279210 _ trimmed.pdf\n",
      "2025-08-27 11:00:53,175 | INFO | Output directory: ./Output/202505 - TX - PRGS-134279210 _ trimmed\n",
      "2025-08-27 11:00:53,176 | INFO | --- Skipping Step 1, Loading pre-classified data from: /Users/jake/Documents/Python/PDF Parser/Output/202505 - TX - PRGS-134279210 _ trimmed/classified_pages.csv ---\n",
      "2025-08-27 11:00:53,265 | INFO | Extracted Company Name from CSV: Progressive County Mutual Insurance Company\n",
      "2025-08-27 11:00:53,265 | INFO | \n",
      "--- Starting Step 2: Factor Table Extraction ---\n",
      "2025-08-27 11:00:53,266 | INFO | Successfully loaded OpenAI API key.\n",
      "2025-08-27 11:00:53,309 | INFO | Processing a potential table group spanning pages: [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "# --- Main Script Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- USER INPUT ---\n",
    "    input_pdf_path = \"./202505 - TX - PRGS-134279210 _ trimmed.pdf\"\n",
    "    # To skip the classification step and use an existing CSV, provide the path here.\n",
    "    # Otherwise, leave it as None.\n",
    "    # Example: pre_classified_csv_path = \"./Output/PGR_Ohio_BNIC-134120828/classified_pages.csv\"\n",
    "    pre_classified_csv_path = \"/Users/jake/Documents/Python/PDF Parser/Output/202505 - TX - PRGS-134279210 _ trimmed/classified_pages.csv\" # <--- SET THIS TO A CSV PATH TO SKIP STEP 1\n",
    "\n",
    "    # --- DYNAMIC OUTPUT PATHS ---\n",
    "    base_filename = os.path.basename(input_pdf_path)\n",
    "    file_name_without_ext = os.path.splitext(base_filename)[0]\n",
    "    \n",
    "    # 1. Create a dedicated output subfolder for the file\n",
    "    output_dir = os.path.join(\"./Output\", file_name_without_ext)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define output paths inside the new subfolder\n",
    "    output_csv_path = os.path.join(output_dir, f\"classified_pages.csv\")\n",
    "    output_excel_path = os.path.join(output_dir, f\"extracted_factor_tables.xlsx\")\n",
    "    \n",
    "    logger.info(f\"Input PDF: {input_pdf_path}\")\n",
    "    logger.info(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    # --- SCRIPT LOGIC ---\n",
    "    # 2. Run the two main steps sequentially, with an option to skip Step 1\n",
    "    \n",
    "    # Check if a pre-classified CSV should be used\n",
    "    if pre_classified_csv_path and os.path.exists(pre_classified_csv_path):\n",
    "        logger.info(f\"--- Skipping Step 1, Loading pre-classified data from: {pre_classified_csv_path} ---\")\n",
    "        classified_data = pd.read_csv(pre_classified_csv_path)\n",
    "        # Handle potential NaN values in the 'text' column when loading from CSV\n",
    "        classified_data['text'] = classified_data['text'].fillna('')\n",
    "        company = extract_company_name(classified_data)\n",
    "        logger.info(f\"Extracted Company Name from CSV: {company}\")\n",
    "    else:\n",
    "        if pre_classified_csv_path:\n",
    "            logger.warning(f\"Pre-classified file not found at '{pre_classified_csv_path}'. Running full classification process.\")\n",
    "        # Run the full classification step if no pre-classified file is provided\n",
    "        classified_data, company = run_classification_step(input_pdf_path, output_csv_path)\n",
    "\n",
    "    # Run the table extraction step using the (either newly created or loaded) classified data\n",
    "    if not classified_data.empty:\n",
    "        run_table_extraction_step(classified_data, company, output_excel_path)\n",
    "    else:\n",
    "        logger.error(\"Classification data is empty. Cannot proceed to table extraction.\")\n",
    "\n",
    "    logger.info(\"\\n--- Script finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3eb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
