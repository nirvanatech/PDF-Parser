{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/Users/jake/Documents/Key/OPENAI_KEY.txt', 'r') as f:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = f.read().strip()\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Set your API key\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # or paste your key as a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EDIT FOR TESTING ---\n",
    "TEST_PDF_PATH = \"./Inputs/PGR_Ohio_BNIC-134120828_trimmed.pdf\"\n",
    "TEST_PAGE_NUMBERS = [31, 41, 51]  # 1-based\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "BUCKETS = [\"intro information\", \"correspondence\", \"rule\", \"factor table\", \"actuarial support\"]\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\n",
    "You are an expert insurance regulatory analyst.\n",
    "Given a single page of an insurance rate or form filing, classify it strictly into one of the following categories: {BUCKETS}.\n",
    "Return just the most likely bucket (as a lowercase label) and a confidence score (0 to 1, 1 being highest confidence).\n",
    "If unsure, choose 'other' as the label.\n",
    "\n",
    "RESPONSE FORMAT (json, on one line): {{\"bucket\": \"...\", \"confidence\": 0.95}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d413ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_single_page(pdf_path, page_number):\n",
    "    \"\"\"page_number is 1-based for user-friendliness\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        n = len(reader.pages)\n",
    "        if not (1 <= page_number <= n):\n",
    "            raise ValueError(f\"Page number {page_number} out of bounds. Document has {n} pages.\")\n",
    "        text = reader.pages[page_number - 1].extract_text() or \"\"\n",
    "        return text\n",
    "\n",
    "def classify_page_with_confidence(text, client, model=MODEL,\n",
    "                                  system_prompt=SYS_PROMPT\n",
    "                                  , temperature=0):\n",
    "    prompt = f\"\"\"Page content:\n",
    "\\\"\\\"\\\"\n",
    "{text[:4000]}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Label the above page strictly. Remember: RESPONSE FORMAT (json, one line): {{\"bucket\": \"...\", \"confidence\": <float>}}\n",
    "\"\"\"\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=64\n",
    "    )\n",
    "    raw_result = chat_response.choices[0].message.content.strip()\n",
    "    # Parse JSON from model output\n",
    "    import json\n",
    "    try:\n",
    "        result = json.loads(raw_result)\n",
    "    except Exception:\n",
    "        # fallback: very basic extraction\n",
    "        import re\n",
    "        label = re.search(r'\"bucket\"\\s*:\\s*\"([^\"]+)\"', raw_result)\n",
    "        conf = re.search(r'\"confidence\"\\s*:\\s*([0-9.]+)', raw_result)\n",
    "        result = {\"bucket\": label.group(1) if label else \"parse_error\", \"confidence\": float(conf.group(1)) if conf else 0.5}\n",
    "    return result, raw_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e71e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_main():\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    for page_no in TEST_PAGE_NUMBERS:\n",
    "        print(f\"\\n===============================\")\n",
    "        print(f\"Processing page {page_no}...\")\n",
    "        try:\n",
    "            page_text = extract_single_page(TEST_PDF_PATH, page_no)\n",
    "            print(f\"\\n--- Page {page_no}: Preview ---\\n\")\n",
    "            print(page_text[:800].replace(\"\\n\", \" \") + (\"...\" if len(page_text) > 800 else \"\"))\n",
    "            print(\"\\n--- Sending to GPT for classification... ---\\n\")\n",
    "            result, raw_model_output = classify_page_with_confidence(page_text, client)\n",
    "            print(\"--- Model output ---\")\n",
    "            print(f\"Bucket: {result.get('bucket')}\")\n",
    "            print(f\"Confidence: {result.get('confidence')}\")\n",
    "            print(\"Full model reply (for debugging):\")\n",
    "            print(raw_model_output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page_no}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d14689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import PyPDF2\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "LOG_FILENAME = \"auto_label_pages.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILENAME, mode='w', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "PDF_PATH = \"./Inputs/PGR_Ohio_BNIC-134120828.pdf\"\n",
    "API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\"\n",
    "OUTPUT_CSV = \"labeled_pages.csv\"\n",
    "MODEL = \"gpt-4\"\n",
    "N_PARALLEL = 2           # Lower value to avoid OpenAI rate limits\n",
    "MIN_TEXT_LENGTH = 20\n",
    "PAGE_TEXT_SLICE = 1500   # Shorter prompt for stability\n",
    "\n",
    "BUCKETS = [\n",
    "    \"intro information\", \"table of contents\", \"correspondence\", \"rule\", \"factor table\",\n",
    "    \"actuarial support\", \"form\", \"rating example\", \"exhibit\", \"crossed_out\", \"other\", \"llm_new_category\"\n",
    "]\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\n",
    "You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "Your job is to classify each page into one of several specific \"buckets\" using explicit criteria and examples below.\n",
    "If the page is blank or contains no meaningful text, select \"other\". If the page has been fully crossed/striked out, select \"crossed_out\". If none fit, invent a new label as \"llm_new_category\" with a 10-word description.\n",
    "BUCKETS:\n",
    "1. \"intro information\": Cover letters, summaries, company info.\n",
    "2. \"table of contents\": Index/table of rules or forms.\n",
    "3. \"correspondence\": Letters, memos, state DOI communication.\n",
    "4. \"rule\": Rating rules/guidelines/policies.\n",
    "5. \"factor table\": Rate/rating factor tables.\n",
    "6. \"actuarial support\": Math, trends, loss ratios, actuarial exhibits.\n",
    "7. \"form\": Forms/endorsements/policy wording.\n",
    "8. \"rating example\": Sample calculations, worked examples.\n",
    "9. \"exhibit\": Charts, graphs, maps, attachments.\n",
    "10. \"crossed_out\": Page is withdrawn, crossed out or all strikethrough.\n",
    "11. \"other\": Clearly none of the above.\n",
    "12. \"llm_new_category\": If new type, add a 10-word explanation.\n",
    "If uncertain, choose \"other\". Only one bucket per page. Reply in JSON: {{\"bucket\": \"rule\", \"confidence\": 0.93}}\n",
    "\"\"\"\n",
    "\n",
    "# --- Util ---\n",
    "def extract_pdf_pages(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        return [page.extract_text() or \"\" for page in reader.pages]\n",
    "\n",
    "def extract_single_page(pdf_path, page_number):  # 1-based\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        n = len(reader.pages)\n",
    "        if not (1 <= page_number <= n):\n",
    "            raise ValueError(f\"Page number {page_number} out of bounds. Document has {n} pages.\")\n",
    "        text = reader.pages[page_number - 1].extract_text()\n",
    "        return text or \"\"\n",
    "\n",
    "with open(API_KEY_PATH) as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def classify_page_gpt(page_num, text, model=MODEL, max_page_text_len=PAGE_TEXT_SLICE, verbose=False, retries=3):\n",
    "    if len((text or '').strip()) < MIN_TEXT_LENGTH:\n",
    "        if verbose:\n",
    "            logger.info(f\"Page {page_num + 1}: Skipped (empty or too short) [other]\")\n",
    "        return {\n",
    "            \"page_number\": page_num+1,\n",
    "            \"page_text\": (text or \"\")[:300],\n",
    "            \"gpt_bucket_guess\": \"other\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"llm_new_category_description\": \"\"\n",
    "        }\n",
    "    prompt = f'Page content:\\n\"\"\"\\n{text[:max_page_text_len]}\\n\"\"\"\\nClassify as per instructions.'\n",
    "    for attempt in range(1, retries+1):\n",
    "        if verbose:\n",
    "            print(\"=\"*40)\n",
    "            print(f\"[GPT TEST] Page {page_num+1} | Attempt {attempt} | Length: {len(text)} | Prompt len: {len(prompt)}\")\n",
    "            print(prompt[:400])\n",
    "            print(\"=\"*40)\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=80\n",
    "            )\n",
    "            reply = response.choices[0].message.content.strip().replace('\\n', ' ')\n",
    "            import json, re\n",
    "            try:\n",
    "                parsed = json.loads(reply)\n",
    "            except Exception:\n",
    "                m = re.search(r'\"bucket\"\\s*:\\s*\"([^\"]+)\"', reply)\n",
    "                m2 = re.search(r'\"confidence\"\\s*:\\s*([0-9.]+)', reply)\n",
    "                parsed = {\n",
    "                    \"bucket\": m.group(1) if m else \"parse_error\",\n",
    "                    \"confidence\": float(m2.group(1)) if m2 else 0.5\n",
    "                }\n",
    "            desc = \"\"\n",
    "            if parsed.get(\"bucket\", \"\").startswith(\"llm_new_category\") and \"|\" in reply:\n",
    "                desc = reply.split(\"|\",1)[1].strip()\n",
    "            if verbose:\n",
    "                logger.info(f'Page {page_num + 1:>4}: [{parsed.get(\"bucket\")}] (conf: {parsed.get(\"confidence\")})')\n",
    "                if desc:\n",
    "                    logger.info(f\"    LLM new category description: {desc}\")\n",
    "            return {\n",
    "                \"page_number\": page_num+1,\n",
    "                \"page_text\": text[:300],\n",
    "                \"gpt_bucket_guess\": parsed.get(\"bucket\", \"\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", 0.5),\n",
    "                \"llm_new_category_description\": desc\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f'Page {page_num + 1}: API ERROR: {type(e).__name__}: {e} [attempt {attempt}]')\n",
    "            if attempt < retries:\n",
    "                wait = 3 * attempt\n",
    "                logger.info(f\"Retrying page {page_num + 1} after {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                return {\n",
    "                    \"page_number\": page_num+1,\n",
    "                    \"page_text\": text[:300],\n",
    "                    \"gpt_bucket_guess\": f\"API_ERROR_{type(e).__name__}\",\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"llm_new_category_description\": \"\"\n",
    "                }\n",
    "\n",
    "def auto_label_pages_serial(pdf_path, output_csv, verbose=False):\n",
    "    try:\n",
    "        pages = extract_pdf_pages(pdf_path)\n",
    "        total_pages = len(pages)\n",
    "        logger.info(f\"PDF extracted successfully. Number of pages: {total_pages}\")\n",
    "        if total_pages == 0:\n",
    "            logger.error(f\"No pages found in PDF: {pdf_path}\")\n",
    "            return []\n",
    "        logger.info(f\"First page text preview: '{(pages[0][:100] if pages[0] else '[Empty page]')}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not extract PDF pages from {pdf_path}: {e}\")\n",
    "        return []\n",
    "    logger.info(f\"Classifying {total_pages} pages from: {pdf_path}\")\n",
    "    results = []\n",
    "    with open(output_csv, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\n",
    "            \"page_number\", \"page_text\", \"gpt_bucket_guess\", \"confidence\", \"llm_new_category_description\"\n",
    "        ]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i, text in tqdm(list(enumerate(pages)), desc=\"Classifying\", total=total_pages):\n",
    "            row = classify_page_gpt(i, text, verbose=verbose)\n",
    "            writer.writerow(row)\n",
    "            results.append(row)\n",
    "            csvfile.flush()\n",
    "            if i % 5 == 0 or i+1 == total_pages:\n",
    "                logger.info(f\"Processed {i+1} of {total_pages} pages\")\n",
    "    logger.info(\"Classification complete. Review your output CSV and log files.\")\n",
    "    return results\n",
    "\n",
    "def auto_label_pages_parallel(pdf_path, output_csv, n_parallel=2, verbose=False):\n",
    "    try:\n",
    "        pages = extract_pdf_pages(pdf_path)\n",
    "        total_pages = len(pages)\n",
    "        logger.info(f\"PDF extracted successfully. Number of pages: {total_pages}\")\n",
    "        if total_pages == 0:\n",
    "            logger.error(f\"No pages found in PDF: {pdf_path}\")\n",
    "            return []\n",
    "        logger.info(f\"First page text preview: '{(pages[0][:100] if pages[0] else '[Empty page]')}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not extract PDF pages from {pdf_path}: {e}\")\n",
    "        return []\n",
    "    logger.info(f\"Classifying {total_pages} pages from: {pdf_path}\")\n",
    "    results = [None] * total_pages\n",
    "    with open(output_csv, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\n",
    "            \"page_number\", \"page_text\", \"gpt_bucket_guess\", \"confidence\", \"llm_new_category_description\"\n",
    "        ]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        with ThreadPoolExecutor(max_workers=n_parallel) as executor:\n",
    "            fut_to_idx = {executor.submit(classify_page_gpt, i, text, verbose=verbose): i for i, text in enumerate(pages)}\n",
    "            completed = 0\n",
    "            for fut in tqdm(as_completed(fut_to_idx), total=len(fut_to_idx), desc=\"Classifying\"):\n",
    "                idx = fut_to_idx[fut]\n",
    "                try:\n",
    "                    row = fut.result()\n",
    "                    if verbose:\n",
    "                        logger.info(f\"Page {row['page_number']}: bucket='{row['gpt_bucket_guess']}', confidence={row['confidence']}, text preview='{row['page_text'][:80]}'\")\n",
    "                        if row.get(\"llm_new_category_description\"):\n",
    "                            logger.info(f\"    New LLM category desc: {row['llm_new_category_description']}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error classifying page {idx+1}: {e}\")\n",
    "                    row = {\n",
    "                        \"page_number\": idx+1,\n",
    "                        \"page_text\": \"\",\n",
    "                        \"gpt_bucket_guess\": \"classification_error\",\n",
    "                        \"confidence\": 0.0,\n",
    "                        \"llm_new_category_description\": \"\"\n",
    "                    }\n",
    "                results[idx] = row\n",
    "                writer.writerow(row)\n",
    "                csvfile.flush()\n",
    "                completed += 1\n",
    "                if completed % 5 == 0 or completed == total_pages:\n",
    "                    logger.info(f\"Processed {completed} of {total_pages} pages\")\n",
    "    logger.info(\"Classification complete. Review your output CSV and log files.\")\n",
    "    return results\n",
    "\n",
    "def list_openai_models():\n",
    "    print(\"Available OpenAI models:\")\n",
    "    models = client.models.list()\n",
    "    for m in models.data:\n",
    "        print(m.id)\n",
    "\n",
    "def pdf_test_only_count_and_preview(path=PDF_PATH, n_preview=3):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(reader.pages)\n",
    "            print(f\"PDF opened! Page count: {num_pages}\")\n",
    "            for i in range(min(n_preview, num_pages)):\n",
    "                page = reader.pages[i]\n",
    "                txt = page.extract_text() or \"[NO TEXT FOUND]\"\n",
    "                print(f\"\\nPage {i+1} text preview: {txt[:300]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"PDF open/read error: {e}\")\n",
    "\n",
    "def test_single_page(page_number=1, verbose=True):\n",
    "    logger.info(f\"Testing classification for page {page_number}\")\n",
    "    text = extract_single_page(PDF_PATH, page_number)\n",
    "    print(\"\\n--- Extracted text preview ---\\n\", repr(text[:500]))\n",
    "    row = classify_page_gpt(page_number-1, text, verbose=verbose)\n",
    "    print(\"\\n--- Classification result ---\\n\", row)\n",
    "    return row\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Select a mode: ---\n",
    "    # Options: \"model_list\", \"pdf_preview\", \"single_page\", \"full_run_serial\", \"full_run_parallel\"\n",
    "    TEST_MODE = \"full_run_parallel\"\n",
    "    TEST_PAGE_NUMBER = 31  # for single_page\n",
    "\n",
    "    if TEST_MODE == \"model_list\":\n",
    "        list_openai_models()\n",
    "    elif TEST_MODE == \"pdf_preview\":\n",
    "        pdf_test_only_count_and_preview(PDF_PATH, n_preview=5)\n",
    "    elif TEST_MODE == \"single_page\":\n",
    "        logger.info(f\"Reading PDF: {PDF_PATH}\")\n",
    "        logger.info(f\"Writing CSV: {OUTPUT_CSV}\")\n",
    "        t0 = time.time()\n",
    "        test_single_page(TEST_PAGE_NUMBER, verbose=True)\n",
    "        logger.info(f\"Single page test done in {time.time() - t0:.2f}s.\")\n",
    "    elif TEST_MODE == \"full_run_serial\":\n",
    "        logger.info(f\"Reading PDF: {PDF_PATH}\")\n",
    "        logger.info(f\"Writing CSV: {OUTPUT_CSV}\")\n",
    "        auto_label_pages_serial(PDF_PATH, OUTPUT_CSV, verbose=True)\n",
    "        logger.info(\"Full PDF labeling (serial) done.\")\n",
    "    elif TEST_MODE == \"full_run_parallel\":\n",
    "        logger.info(f\"Reading PDF: {PDF_PATH}\")\n",
    "        logger.info(f\"Writing CSV: {OUTPUT_CSV}\")\n",
    "        auto_label_pages_parallel(PDF_PATH, OUTPUT_CSV, n_parallel=N_PARALLEL, verbose=True)\n",
    "        logger.info(\"Full PDF labeling (parallel) done.\")\n",
    "\n",
    "    logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195079f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This code snippet tests if the OpenAI API key is set correctly and returns a simple response.\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"\")  # Use your real key\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Say ONLY: test OK\"},\n",
    "        {\"role\": \"user\", \"content\": \"Test page contents\"}\n",
    "    ],\n",
    "    max_tokens=10\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# This code snippet lists all available OpenAI models using the OpenAI Python client.\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\"\n",
    "with open(API_KEY_PATH) as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import PyPDF2\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "PDF_PATH = \"./Inputs/PGR_Ohio_BNIC-134120828.pdf\"\n",
    "API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\"\n",
    "OUTPUT_CSV = \"labeled_pages.csv\"\n",
    "MODEL = \"gpt-4\"  # or \"gpt-3.5-turbo\"\n",
    "MIN_TEXT_LENGTH = 20\n",
    "PAGE_TEXT_SLICE = 1500\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "Classify each page into one of several explicit \"buckets\": ...\n",
    "(Truncate for brevity. Use your detailed prompt here.)\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "def extract_pdf_pages(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "            print(f\"extract_pdf_pages: Loaded {len(pages)} pages from {pdf_path}\")\n",
    "            logger.info(f\"extract_pdf_pages: Loaded {len(pages)} pages from {pdf_path}\")\n",
    "            return pages\n",
    "    except Exception as e:\n",
    "        print(f\"extract_pdf_pages ERROR: {e}\")\n",
    "        logger.error(f\"extract_pdf_pages ERROR: {e}\")\n",
    "        raise\n",
    "\n",
    "with open(API_KEY_PATH) as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def classify_page_gpt(page_num, text, model=MODEL, max_page_text_len=PAGE_TEXT_SLICE, retries=3):\n",
    "    if len((text or '').strip()) < MIN_TEXT_LENGTH:\n",
    "        print(f\"Page {page_num + 1}: Skipped (empty or too short) [other]\")\n",
    "        logger.info(f\"Page {page_num + 1}: Skipped (empty or too short) [other]\")\n",
    "        return {\n",
    "            \"page_number\": page_num+1,\n",
    "            \"page_text\": (text or \"\")[:300],\n",
    "            \"gpt_bucket_guess\": \"other\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"llm_new_category_description\": \"\"\n",
    "        }\n",
    "    prompt = f'Page content:\\n\"\"\"\\n{text[:max_page_text_len]}\\n\"\"\"\\nClassify as per instructions.'\n",
    "    for attempt in range(1, retries+1):\n",
    "        try:\n",
    "            print(f\"Classifying page {page_num+1}, attempt {attempt}, text length {len(text)}\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=80\n",
    "            )\n",
    "            reply = response.choices[0].message.content.strip().replace('\\n', ' ')\n",
    "            import json, re\n",
    "            try:\n",
    "                parsed = json.loads(reply)\n",
    "            except Exception:\n",
    "                m = re.search(r'\"bucket\"\\s*:\\s*\"([^\"]+)\"', reply)\n",
    "                m2 = re.search(r'\"confidence\"\\s*:\\s*([0-9.]+)', reply)\n",
    "                parsed = {\n",
    "                    \"bucket\": m.group(1) if m else \"parse_error\",\n",
    "                    \"confidence\": float(m2.group(1)) if m2 else 0.5\n",
    "                }\n",
    "            desc = \"\"\n",
    "            if parsed.get(\"bucket\", \"\").startswith(\"llm_new_category\") and \"|\" in reply:\n",
    "                desc = reply.split(\"|\",1)[1].strip()\n",
    "            print(f\"Result page {page_num+1}: bucket='{parsed.get('bucket')}', conf={parsed.get('confidence')}\")\n",
    "            logger.info(f\"Result page {page_num+1}: bucket='{parsed.get('bucket')}', conf={parsed.get('confidence')}\")\n",
    "            return {\n",
    "                \"page_number\": page_num+1,\n",
    "                \"page_text\": text[:300],\n",
    "                \"gpt_bucket_guess\": parsed.get(\"bucket\", \"\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", 0.5),\n",
    "                \"llm_new_category_description\": desc\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f'Page {page_num + 1}: API ERROR: {type(e).__name__}: {e} [attempt {attempt}]')\n",
    "            logger.warning(f'Page {page_num + 1}: API ERROR: {type(e).__name__}: {e} [attempt {attempt}]')\n",
    "            if attempt < retries:\n",
    "                wait = 3 * attempt\n",
    "                print(f\"Retrying page {page_num + 1} after {wait}s...\")\n",
    "                logger.info(f\"Retrying page {page_num + 1} after {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                return {\n",
    "                    \"page_number\": page_num+1,\n",
    "                    \"page_text\": text[:300],\n",
    "                    \"gpt_bucket_guess\": f\"API_ERROR_{type(e).__name__}\",\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"llm_new_category_description\": \"\"\n",
    "                }\n",
    "\n",
    "\n",
    "def loop_single_page_all(pdf_path, output_csv, max_pages=3):  # Default for test: just 3 pages!\n",
    "    try:\n",
    "        pages = extract_pdf_pages(pdf_path)\n",
    "        total_pages = len(pages)\n",
    "        print(f\"Loaded {total_pages} pages. Will classify up to {max_pages} pages.\")\n",
    "        logger.info(f\"Loaded {total_pages} pages. Will classify up to {max_pages} pages.\")\n",
    "        with open(output_csv, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "            fieldnames = [\n",
    "                \"page_number\", \"page_text\", \"gpt_bucket_guess\", \"confidence\", \"llm_new_category_description\"\n",
    "            ]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for i, text in enumerate(pages[:max_pages]):\n",
    "                t0 = time.time()\n",
    "                print(f\"\\n--- About to classify page {i+1}/{total_pages} ---\")\n",
    "                logger.info(f\"\\n--- About to classify page {i+1}/{total_pages} ---\")\n",
    "                row = classify_page_gpt(i, text)\n",
    "                writer.writerow(row)\n",
    "                csvfile.flush()\n",
    "                print(f\"--- Done with page {i+1}: bucket={row['gpt_bucket_guess']}, confidence={row['confidence']}, elapsed {time.time()-t0:.2f}s ---\\n\")\n",
    "                logger.info(f\"--- Done with page {i+1}: bucket={row['gpt_bucket_guess']}, confidence={row['confidence']}, elapsed {time.time()-t0:.2f}s ---\")\n",
    "        print(\"Loop finished. Check output CSV for results.\")\n",
    "        logger.info(\"Loop finished. Check output CSV for results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"SCRIPT ERROR: {e}\")\n",
    "        logger.error(f\"SCRIPT ERROR: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loop_single_page_all(PDF_PATH, OUTPUT_CSV, max_pages=3)  # Start with 3 for fast feedback; set to None or remove for all once happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ada00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PDF extraction only.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting PDF extraction only.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     pages = \u001b[43mextract_pdf_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDF_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pages.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtest_page_texts.csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, newline=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mextract_pdf_pages\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pdf_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     31\u001b[39m     reader = PyPDF2.PdfReader(f)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     pages = [\u001b[43mpage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m reader.pages]\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pages from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pages from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1851\u001b[39m, in \u001b[36mPageObject.extract_text\u001b[39m\u001b[34m(self, Tj_sep, TJ_sep, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, *args)\u001b[39m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   1849\u001b[39m     orientations = (orientations,)\n\u001b[32m-> \u001b[39m\u001b[32m1851\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1737\u001b[39m, in \u001b[36mPageObject._extract_text\u001b[39m\u001b[34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1734\u001b[39m xobj = resources_dict[\u001b[33m\"\u001b[39m\u001b[33m/XObject\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xobj[operands[\u001b[32m0\u001b[39m]][\u001b[33m\"\u001b[39m\u001b[33m/Subtype\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33m/Image\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1736\u001b[39m     \u001b[38;5;66;03m# output += text\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_xform_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1739\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1745\u001b[39m     output += text\n\u001b[32m   1746\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m visitor_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1880\u001b[39m, in \u001b[36mPageObject.extract_xform_text\u001b[39m\u001b[34m(self, xform, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_xform_text\u001b[39m(\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1864\u001b[39m     xform: EncodedStreamObject,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1869\u001b[39m     visitor_text: Optional[Callable[[Any, Any, Any, Any, Any], \u001b[38;5;28;01mNone\u001b[39;00m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1870\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1871\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1872\u001b[39m \u001b[33;03m    Extract text from an XObject.\u001b[39;00m\n\u001b[32m   1873\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1878\u001b[39m \u001b[33;03m        The extracted text\u001b[39;00m\n\u001b[32m   1879\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1737\u001b[39m, in \u001b[36mPageObject._extract_text\u001b[39m\u001b[34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1734\u001b[39m xobj = resources_dict[\u001b[33m\"\u001b[39m\u001b[33m/XObject\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xobj[operands[\u001b[32m0\u001b[39m]][\u001b[33m\"\u001b[39m\u001b[33m/Subtype\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33m/Image\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1736\u001b[39m     \u001b[38;5;66;03m# output += text\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_xform_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1739\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1745\u001b[39m     output += text\n\u001b[32m   1746\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m visitor_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1880\u001b[39m, in \u001b[36mPageObject.extract_xform_text\u001b[39m\u001b[34m(self, xform, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_xform_text\u001b[39m(\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1864\u001b[39m     xform: EncodedStreamObject,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1869\u001b[39m     visitor_text: Optional[Callable[[Any, Any, Any, Any, Any], \u001b[38;5;28;01mNone\u001b[39;00m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1870\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1871\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1872\u001b[39m \u001b[33;03m    Extract text from an XObject.\u001b[39;00m\n\u001b[32m   1873\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1878\u001b[39m \u001b[33;03m        The extracted text\u001b[39;00m\n\u001b[32m   1879\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: PageObject._extract_text at line 1737 (1 times), PageObject.extract_xform_text at line 1880 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1737\u001b[39m, in \u001b[36mPageObject._extract_text\u001b[39m\u001b[34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1734\u001b[39m xobj = resources_dict[\u001b[33m\"\u001b[39m\u001b[33m/XObject\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xobj[operands[\u001b[32m0\u001b[39m]][\u001b[33m\"\u001b[39m\u001b[33m/Subtype\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33m/Image\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1736\u001b[39m     \u001b[38;5;66;03m# output += text\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_xform_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1739\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1745\u001b[39m     output += text\n\u001b[32m   1746\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m visitor_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1880\u001b[39m, in \u001b[36mPageObject.extract_xform_text\u001b[39m\u001b[34m(self, xform, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_xform_text\u001b[39m(\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1864\u001b[39m     xform: EncodedStreamObject,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1869\u001b[39m     visitor_text: Optional[Callable[[Any, Any, Any, Any, Any], \u001b[38;5;28;01mNone\u001b[39;00m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1870\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1871\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1872\u001b[39m \u001b[33;03m    Extract text from an XObject.\u001b[39;00m\n\u001b[32m   1873\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1878\u001b[39m \u001b[33;03m        The extracted text\u001b[39;00m\n\u001b[32m   1879\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m        \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/_page.py:1356\u001b[39m, in \u001b[36mPageObject._extract_text\u001b[39m\u001b[34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   1352\u001b[39m     content = (\n\u001b[32m   1353\u001b[39m         obj[content_key].get_object() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[32m   1354\u001b[39m     )\n\u001b[32m   1355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, ContentStream):\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m         content = \u001b[43mContentStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbytes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:  \u001b[38;5;66;03m# it means no content can be extracted(certainly empty page)\u001b[39;00m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/generic/_data_structures.py:877\u001b[39m, in \u001b[36mContentStream.__init__\u001b[39m\u001b[34m(self, stream, pdf, forced_encoding)\u001b[39m\n\u001b[32m    875\u001b[39m     stream_bytes = BytesIO(stream_data_bytes)\n\u001b[32m    876\u001b[39m \u001b[38;5;28mself\u001b[39m.forced_encoding = forced_encoding\n\u001b[32m--> \u001b[39m\u001b[32m877\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__parse_content_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/PyPDF2/generic/_data_structures.py:940\u001b[39m, in \u001b[36mContentStream.__parse_content_stream\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m    933\u001b[39m         operands = []\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m peek == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    935\u001b[39m     \u001b[38;5;66;03m# If we encounter a comment in the content stream, we have to\u001b[39;00m\n\u001b[32m    936\u001b[39m     \u001b[38;5;66;03m# handle it here.  Typically, read_object will handle\u001b[39;00m\n\u001b[32m    937\u001b[39m     \u001b[38;5;66;03m# encountering a comment -- but read_object assumes that\u001b[39;00m\n\u001b[32m    938\u001b[39m     \u001b[38;5;66;03m# following the comment must be the object we're trying to\u001b[39;00m\n\u001b[32m    939\u001b[39m     \u001b[38;5;66;03m# read.  In this case, it could be an operator instead.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m peek \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m    941\u001b[39m         peek = stream.read(\u001b[32m1\u001b[39m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#  this script is a more robust version of the previous one, with better error handling and logging.\n",
    "import os\n",
    "import csv\n",
    "import PyPDF2\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# === LOGGING SETUP ===\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "PDF_PATH = \"./Inputs/PGR_Ohio_BNIC-134120828_trimmed.pdf\"\n",
    "API_KEY_PATH = \"/Users/jake/Documents/Key/OPENAI_KEY.txt\"\n",
    "OUTPUT_CSV = \"labeled_pages.csv\"\n",
    "MODEL = \"gpt-4o\"\n",
    "MIN_TEXT_LENGTH = 20\n",
    "PAGE_TEXT_SLICE = 1500\n",
    "SECONDS_BETWEEN_PAGES = 5     # <-- adjust as you wish!\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are an expert insurance regulatory analyst reviewing a state commercial auto insurance rate and rule filing.\n",
    "Classify each page into one of several explicit \"buckets\": intro information, table of contents, correspondence, rule, factor table, actuarial support, form, rating example, exhibit, crossed_out, other, or llm_new_category (with a 10-word description if used). Return a JSON like {\"bucket\": \"...\", \"confidence\": 0.93}. Only one bucket per page. If uncertain, pick \"other\".\n",
    "\"\"\"\n",
    "\n",
    "def extract_pdf_pages(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "        print(f\"Extracted {len(pages)} pages from {pdf_path}\")\n",
    "        logger.info(f\"Extracted {len(pages)} pages from {pdf_path}\")\n",
    "        return pages\n",
    "\n",
    "with open(API_KEY_PATH) as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def classify_page_gpt(page_num, text, model=MODEL, max_page_text_len=PAGE_TEXT_SLICE, retries=3):\n",
    "    if len((text or '').strip()) < MIN_TEXT_LENGTH:\n",
    "        print(f\"Page {page_num+1}: Skipped (empty or too short) [other]\")\n",
    "        logger.info(f\"Page {page_num+1}: Skipped (empty or too short) [other]\")\n",
    "        return {\n",
    "            \"page_number\": page_num+1,\n",
    "            \"page_text\": (text or \"\")[:300],\n",
    "            \"gpt_bucket_guess\": \"other\",\n",
    "            \"confidence\": 1.0,\n",
    "            \"llm_new_category_description\": \"\"\n",
    "        }\n",
    "    \n",
    "    prompt = f'Page content:\\n\"\"\"\\n{text[:max_page_text_len]}\\n\"\"\"\\nClassify as per instructions.'\n",
    "    for attempt in range(1, retries+1):\n",
    "        try:\n",
    "            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] About to classify page {page_num+1}, attempt {attempt}.\")\n",
    "            logger.info(f\"About to classify page {page_num+1}, attempt {attempt}.\")\n",
    "            start_api = time.time()\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=80\n",
    "            )\n",
    "            elapsed_api = time.time() - start_api\n",
    "            reply = response.choices[0].message.content.strip().replace('\\n', ' ')\n",
    "            import json, re\n",
    "            try:\n",
    "                parsed = json.loads(reply)\n",
    "            except Exception:\n",
    "                m = re.search(r'\"bucket\"\\s*:\\s*\"([^\"]+)\"', reply)\n",
    "                m2 = re.search(r'\"confidence\"\\s*:\\s*([0-9.]+)', reply)\n",
    "                parsed = {\n",
    "                    \"bucket\": m.group(1) if m else \"parse_error\",\n",
    "                    \"confidence\": float(m2.group(1)) if m2 else 0.5\n",
    "                }\n",
    "            desc = \"\"\n",
    "            if parsed.get(\"bucket\", \"\").startswith(\"llm_new_category\") and \"|\" in reply:\n",
    "                desc = reply.split(\"|\",1)[1].strip()\n",
    "            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Got result for page {page_num+1} after {elapsed_api:.2f}s: bucket='{parsed.get('bucket')}', conf={parsed.get('confidence')}\")\n",
    "            logger.info(f\"Got result for page {page_num+1} after {elapsed_api:.2f}s: bucket='{parsed.get('bucket')}', conf={parsed.get('confidence')}\")\n",
    "            return {\n",
    "                \"page_number\": page_num+1,\n",
    "                \"page_text\": text[:300],\n",
    "                \"gpt_bucket_guess\": parsed.get(\"bucket\", \"\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", 0.5),\n",
    "                \"llm_new_category_description\": desc\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Page {page_num + 1}: API ERROR: {type(e).__name__}: {e} [attempt {attempt}]\")\n",
    "            logger.warning(f\"Page {page_num + 1}: API ERROR: {type(e).__name__}: {e} [attempt {attempt}]\")\n",
    "            if attempt < retries:\n",
    "                wait = 3 * attempt\n",
    "                print(f\"Retrying page {page_num + 1} after {wait}s...\")\n",
    "                logger.info(f\"Retrying page {page_num + 1} after {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                return {\n",
    "                    \"page_number\": page_num+1,\n",
    "                    \"page_text\": text[:300],\n",
    "                    \"gpt_bucket_guess\": f\"API_ERROR_{type(e).__name__}\",\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"llm_new_category_description\": \"\"\n",
    "                }\n",
    "\n",
    "def loop_single_page_all(pdf_path, output_csv, seconds_between_pages=SECONDS_BETWEEN_PAGES):\n",
    "    pages = extract_pdf_pages(pdf_path)\n",
    "    total_pages = len(pages)\n",
    "    print(f\"Loaded {total_pages} pages.\")\n",
    "    logger.info(f\"Loaded {total_pages} pages.\")\n",
    "    with open(output_csv, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\n",
    "            \"page_number\", \"page_text\", \"gpt_bucket_guess\", \"confidence\", \"llm_new_category_description\"\n",
    "        ]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i, text in enumerate(pages):\n",
    "            t0 = time.time()\n",
    "            print(f\"\\n--- Progress: {i+1}/{total_pages} pages ({int(100*(i+1)/total_pages)}%) ---\")\n",
    "            logger.info(f\"Progress: {i+1}/{total_pages} ({int(100*(i+1)/total_pages)}%)\")\n",
    "            row = classify_page_gpt(i, text)\n",
    "            writer.writerow(row)\n",
    "            csvfile.flush()\n",
    "            elapsed = time.time()-t0\n",
    "            print(f\"--- Done page {i+1}: bucket={row['gpt_bucket_guess']}, confidence={row['confidence']}, elapsed {elapsed:.2f}s ---\")\n",
    "            logger.info(f\"Done page {i+1}: bucket={row['gpt_bucket_guess']}, confidence={row['confidence']}, elapsed {elapsed:.2f}s\")\n",
    "            # Emit a progress checkpoint every 5 pages\n",
    "            if (i+1) % 5 == 0 or i == total_pages-1:\n",
    "                print(f\"[Checkpoint] Classified {i+1} of {total_pages} pages ({int(100*(i+1)/total_pages)}%)\")\n",
    "                logger.info(f\"[Checkpoint] Classified {i+1} of {total_pages} pages ({int(100*(i+1)/total_pages)}%)\")\n",
    "            if i < total_pages - 1:\n",
    "                print(f\"Sleeping {seconds_between_pages} seconds before next page.\")\n",
    "                time.sleep(seconds_between_pages)\n",
    "    print(\"Loop finished. Check output CSV for results.\")\n",
    "    logger.info(\"Loop finished. Check output CSV for results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting PDF extraction only.\")\n",
    "    pages = extract_pdf_pages(PDF_PATH)\n",
    "    print(f\"Extracted {len(pages)} pages.\")\n",
    "    with open(\"test_page_texts.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"page_number\", \"preview\"])\n",
    "        for i, text in enumerate(pages):\n",
    "            writer.writerow([i+1, text[:100].replace('\\n', ' ')])\n",
    "    print(\"Extraction+write done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at your_document.pdf\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783ac98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
